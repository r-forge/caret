



<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
  <!--
  Design by Free CSS Templates
http://www.freecsstemplates.org
Released for free under a Creative Commons Attribution 2.5 License

Name       : Emerald 
Description: A two-column, fixed-width design with dark color scheme.
Version    : 1.0
Released   : 20120902

-->
  <html xmlns="http://www.w3.org/1999/xhtml">
  <head>
<style type="text/css">.knitr.inline {
  background-color: #f7f7f7;
  border:solid 1px #B0B0B0;
}
.error {
	font-weight: bold;
	color: #FF0000;
},
.warning {
	font-weight: bold;
}
.message {
	font-style: italic;
}
.source, .output, .warning, .error, .message {
	padding: 0em 1em;
  border:solid 1px #F7F7F7;
}
.source {
  background-color: #f5f5f5;
}
.rimage.left {
  text-align: left;
}
.rimage.right {
  text-align: right;
}
.rimage.center {
  text-align: center;
}
.hl.num {
  color: #AF0F91;
}
.hl.str {
  color: #317ECC;
}
.hl.com {
  color: #AD95AF;
  font-style: italic;
}
.hl.opt {
  color: #000000;
}
.hl.std {
  color: #585858;
}
.hl.kwa {
  color: #295F94;
  font-weight: bold;
}
.hl.kwb {
  color: #B05A65;
}
.hl.kwc {
  color: #55aa55;
}
.hl.kwd {
  color: #BC5A65;
  font-weight: bold;
}</style>
  <meta name="keywords" content="" />
  <meta name="description" content="" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <title>Miscellaneous Model Functions</title>
  <link href='http://fonts.googleapis.com/css?family=Abel' rel='stylesheet' type='text/css'>
  <link href="style.css" rel="stylesheet" type="text/css" media="screen" />
  </head>
  <body>
  <div id="wrapper">
  <div id="header-wrapper" class="container">
  <div id="header" class="container">
  <div id="logo">
  <h1><a href="#">Miscellaneous Model Functions</a></h1>
</div>
  <!--
  <div id="menu">
  <ul>
  <li class="current_page_item"><a href="#">Homepage</a></li>
<li><a href="#">Blog</a></li>
<li><a href="#">Photos</a></li>
<li><a href="#">About</a></li>
<li><a href="#">Contact</a></li>
</ul>
  </div>
  -->
  </div>
  <div><img src="images/img03.png" width="1000" height="40" alt="" /></div>
  </div>
  <!-- end #header -->
<div id="page">
  <div id="content">
  
<h1>Contents</h1>  
<ul>
  <li><a href="#knn">Yet Another <i>k</i>-Nearest Neighbor Function</a></li>
  <li><a href="#plsda">Partial Least Squares Discriminant Analysis</a></li>
  <li><a href="#bagMARS">Bagged MARS and FDA</a></li>
  <li><a href="#bag">General Purpose Bagging</a></li>
  <li><a href="#avnnet">Model Averaged Neural Networks</a></li>
  <li><a href="#pcannet">Neural Networks with a Principal Component Step</a></li>
  <li><a href="#ica">Independent Component Regression</a></li>
 </ul>   
  
<div id="knn"></div>   
<h1>Yet Another <i>k</i>-Nearest Neighbor Function</h1>

<p><span class="mx funCall">knn3</span> is a function for <i>k</i>-nearest neighbor classification. This particular implementation is a modification of the <span class="mx funCall">knn</span> C code and returns the vote information for all of the classes (<span class="mx funCall">knn</span> only returns the probability for the winning class). There is a formula interface via</p>
  
<div class="chunk" id="unnamed-chunk-2"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">knn3</span><span class="hl std">(formula, data)</span>
<span class="hl com">## or by passing the training data directly x is a matrix or data frame, y is</span>
<span class="hl com">## a factor vector</span>
<span class="hl kwd">knn3</span><span class="hl std">(x, y)</span>
</pre></div>
</div></div>

<p> There are also <code>print</code> and <code>predict</code> methods. </p>

<p>For the Sonar data in the <a href="http://cran.r-project.org/web/packages/mlbench/index.html"><strong>mlbench</strong></a> package, we can fit an 11-nearest neighbor model:</p>

<div class="chunk" id="MiscKnn1"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">library</span><span class="hl std">(caret)</span>
<span class="hl kwd">library</span><span class="hl std">(mlbench)</span>
<span class="hl kwd">data</span><span class="hl std">(Sonar)</span>
<span class="hl kwd">set.seed</span><span class="hl std">(</span><span class="hl num">808</span><span class="hl std">)</span>
<span class="hl std">inTrain</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">createDataPartition</span><span class="hl std">(Sonar</span><span class="hl opt">$</span><span class="hl std">Class,</span> <span class="hl kwc">p</span> <span class="hl std">=</span> <span class="hl num">2</span><span class="hl opt">/</span><span class="hl num">3</span><span class="hl std">,</span> <span class="hl kwc">list</span> <span class="hl std">=</span> <span class="hl num">FALSE</span><span class="hl std">)</span>
<span class="hl com">## Save the predictors and class in different objects</span>
<span class="hl std">sonarTrain</span> <span class="hl kwb">&lt;-</span> <span class="hl std">Sonar[inTrain,</span> <span class="hl opt">-</span><span class="hl kwd">ncol</span><span class="hl std">(Sonar)]</span>
<span class="hl std">sonarTest</span> <span class="hl kwb">&lt;-</span> <span class="hl std">Sonar[</span><span class="hl opt">-</span><span class="hl std">inTrain,</span> <span class="hl opt">-</span><span class="hl kwd">ncol</span><span class="hl std">(Sonar)]</span>
<span class="hl std">trainClass</span> <span class="hl kwb">&lt;-</span> <span class="hl std">Sonar[inTrain,</span> <span class="hl str">&quot;Class&quot;</span><span class="hl std">]</span>
<span class="hl std">testClass</span> <span class="hl kwb">&lt;-</span> <span class="hl std">Sonar[</span><span class="hl opt">-</span><span class="hl std">inTrain,</span> <span class="hl str">&quot;Class&quot;</span><span class="hl std">]</span>
<span class="hl std">centerScale</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">preProcess</span><span class="hl std">(sonarTrain)</span>
<span class="hl std">centerScale</span>
</pre></div>
<div class="output"><pre class="knitr r">
Call:
preProcess.default(x = sonarTrain)

Created from 139 samples and 60 variables
Pre-processing: centered, scaled 
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl std">training</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">predict</span><span class="hl std">(centerScale, sonarTrain)</span>
<span class="hl std">testing</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">predict</span><span class="hl std">(centerScale, sonarTest)</span>
<span class="hl std">knnFit</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">knn3</span><span class="hl std">(training, trainClass,</span> <span class="hl kwc">k</span> <span class="hl std">=</span> <span class="hl num">11</span><span class="hl std">)</span>
<span class="hl std">knnFit</span>
</pre></div>
<div class="output"><pre class="knitr r">11-nearest neighbor classification model

Call:
knn3.data.frame(x = training, y = trainClass, k = 11)

Training set class distribution:

 M  R 
74 65 
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl kwd">predict</span><span class="hl std">(knnFit,</span> <span class="hl kwd">head</span><span class="hl std">(testing),</span> <span class="hl kwc">type</span> <span class="hl std">=</span> <span class="hl str">&quot;prob&quot;</span><span class="hl std">)</span>
</pre></div>
<div class="output"><pre class="knitr r">          M      R
[1,] 0.2727 0.7273
[2,] 0.2727 0.7273
[3,] 0.1818 0.8182
[4,] 0.1818 0.8182
[5,] 0.5455 0.4545
[6,] 0.0000 1.0000
</pre></div>
</div></div>

 
<p> Similarly, <a href="http://cran.r-project.org/web/packages/caret/index.html"><strong>caret</strong></a> contains a <i>k</i>-nearest neighbor regression function, <span class="mx funCall">knnreg</span>. It returns the average outcome for the neighbor.</p> 

<div id="plsda"></div>  
<h1>Partial Least Squares Discriminant Analysis</h1>

<p>The <span class="mx funCall">plsda</span> function is a wrapper for the <span class="mx funCall">plsr</span> function in the <a href="http://cran.r-project.org/web/packages/pls/index.html"><strong>pls</strong></a> package that does not require a formula interface and can take factor outcomes as arguments. The classes are broken down into dummy variables (one for each class). These 0/1 dummy variables are modeled by partial least squares. 

<p>
From this model, there are two approaches to computing the class predictions and probabilities:
<ul>
   <li>the softmax technique can be used on a per-sample basis to
   normalize the scores so that they are more ``probability like''
   (i.e. they sum to one and are between zero and one). For a vector
   of model predictions for each class <i>X</i>, the softmax class
   probabilities are computed as. The predicted class is simply the class with the largest model prediction, or equivalently, the largest class probability. This is the default behavior for <span class="mx funCall">plsda</span>.</li>
   
   <li>Bayes rule can be applied to the model predictions to form posterior probabilities. Here, the model predictions for the training set are used along with the training set outcomes to create conditional distributions for each class. When new samples are predicted, the raw model predictions are run through these conditional distributions to produce a posterior probability for each class (along with the prior). Bayes rule can be used by specifying <span class="mx arg">probModel</span> = <span class="hl str">&quot;Bayes&quot;</span>. An additional parameter, <span class="mx arg">prior</span>, can be used to set prior probabilities  for the classes. </li>
</ul>
 </p>
<p>The advantage to using Bayes rule is that the full training set is used to directly compute the class probabilities (unlike the softmax function which only uses the current sample's scores). This creates more realistic probability estimates but the disadvantage is that a separate Bayesian model must be created for each value of <span class="mx arg">ncomp</span>, which is more time consuming.  
 </p>
<p>For the sonar data set, we can fit two PLS models using each technique and predict the class probabilities for the test set. 
</p>

<div class="chunk" id="miscpls"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">plsFit</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">plsda</span><span class="hl std">(training, trainClass,</span> <span class="hl kwc">ncomp</span> <span class="hl std">=</span> <span class="hl num">20</span><span class="hl std">)</span>
<span class="hl std">plsFit</span>
</pre></div>
<div class="output"><pre class="knitr r">Partial least squares classification, fitted with the kernel algorithm.
The softmax function was used to compute class probabilities.

Call:
plsr(formula = y ~ x, ncomp = ncomp, data = tmpData)
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl std">plsBayesFit</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">plsda</span><span class="hl std">(training, trainClass,</span> <span class="hl kwc">ncomp</span> <span class="hl std">=</span> <span class="hl num">20</span><span class="hl std">,</span>
                     <span class="hl kwc">probMethod</span> <span class="hl std">=</span> <span class="hl str">&quot;Bayes&quot;</span><span class="hl std">)</span>
<span class="hl std">plsBayesFit</span>
</pre></div>
<div class="output"><pre class="knitr r">Partial least squares classification, fitted with the kernel algorithm.
Bayes rule was used to compute class probabilities.

Call:
plsr(formula = y ~ x, ncomp = ncomp, data = tmpData)
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl kwd">predict</span><span class="hl std">(plsFit,</span> <span class="hl kwd">head</span><span class="hl std">(testing),</span> <span class="hl kwc">type</span> <span class="hl std">=</span> <span class="hl str">&quot;prob&quot;</span><span class="hl std">)</span>
</pre></div>
<div class="output"><pre class="knitr r">, , 20 comps

        M      R
4  0.6228 0.3772
6  0.5241 0.4759
12 0.3884 0.6116
16 0.1925 0.8075
17 0.1801 0.8199
19 0.1337 0.8663
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl kwd">predict</span><span class="hl std">(plsBayesFit,</span> <span class="hl kwd">head</span><span class="hl std">(testing),</span> <span class="hl kwc">type</span> <span class="hl std">=</span> <span class="hl str">&quot;prob&quot;</span><span class="hl std">)</span>
</pre></div>
<div class="output"><pre class="knitr r">, , ncomp20

          M       R
4  0.950775 0.04922
6  0.585469 0.41453
12 0.076099 0.92390
16 0.002769 0.99723
17 0.003715 0.99628
19 0.023820 0.97618
</pre></div>
</div></div>


<p>Similar to <span class="mx funCall">plsda</span>, <a href="http://cran.r-project.org/web/packages/caret/index.html"><strong>caret</strong></a> also contains a function <span class="mx funCall">splsda</span> that allows for classification using sparse PLS. A dummy 
matrix is created for each class and used with the <span class="mx funCall">spls</span> function in the <a href="http://cran.r-project.org/web/packages/spls/index.html"><strong>spls</strong></a> package. The same approach to estimating class probabilities is used for <span class="mx funCall">plsda</span> and <span class="mx funCall">splsda</span>.
</p>

<div id="bagMARS"></div>  
<h1>Bagged MARS and FDA</h1>

<p>
Multivariate adaptive regression splines (MARS) models, like classification/regression trees, are unstable predictors (Breiman, 1996). This means that small perturbations in the training data might lead to significantly different models. Bagged trees and random forests are effective ways of improving tree models by exploiting these instabilities. <a href="http://cran.r-project.org/web/packages/caret/index.html"><strong>caret</strong></a> contains a function, <span class="mx funCall">bagEarth</span>, that fits MARS models via the <span class="mx funCall">earth</span> function. There are formula and non-formula interfaces. 
</p>
<p>
Also, flexible discriminant analysis is a generalization of linear discriminant analysis that can use non-linear features as inputs. One way of doing this is the use MARS-type features to classify samples. The function <span class="mx funCall">bagFDA</span> fits FDA models of a set of bootstrap samples and aggregates the predictions to reduce noise.
</p>
<p>
This function is deprecated in favor of the <span class="mx funCall">bag</span> function.
</p>
<div id="bag"></div>  
<h1>Bagging</h1>

<p>The <span class="mx funCall">bag</span> function offers a general platform for bagging classification and regression models. Like <span class="mx funCall">rfe</span> and <span class="mx funCall">sbf</span>, it is open and models are specified by declaring functions for the model fitting and prediction code (and several built-in sets of functions exist in the package). The function <span class="mx funCall">bagControl</span> has options to specify the functions (more details below).</p>

<p> The function also has a few non-standard features:</p>
   <ul>
      <li> The argument <span class="mx arg">var</span> can enable random sampling of the predictors at each bagging iteration. This is to de-correlate the bagged models in the same spirit of random forests (although here the sampling is done once for the whole model). The default is to use all the predictors for each model.</li>
      <li> The <span class="mx funCall">bagControl</span> function has a logical argument called <span class="mx arg">downSample</span> that is useful for classification models with severe class imbalance. The bootstrapped data set is reduced so that the sample sizes for the classes with larger frequencies are the same as the sample size for the minority class.</li>
      <li>If a parallel backend for the <strong>foreach</strong> package has been loaded and registered, the bagged models can be trained in parallel.</li>
   </ul>
<p>The function's control function requires the following arguments:</p>

<h2>The <span class="mx funCall">fit</span> Function</h2>

<p>Inputs:
<ul>
 <li> <span class="mx arg">x</span>: a data frame of the training set predictor data. 
   <li> <span class="mx arg">y</span>: the training set outcomes.
   <li> <span class="mx arg">...</span> arguments passed from <span class="mx funCall">train</span> to this function
</ul>
</p>
<p>
The output is the object corresponding to the trained model and any
other objects required for prediction. A simple example for a linear discriminant analysis model from the <strong>MASS</strong> package is:
<div class="chunk" id="unnamed-chunk-3"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwa">function</span><span class="hl std">(</span><span class="hl kwc">x</span><span class="hl std">,</span> <span class="hl kwc">y</span><span class="hl std">,</span> <span class="hl kwc">...</span><span class="hl std">) {</span>
   <span class="hl kwd">library</span><span class="hl std">(MASS)</span>
   <span class="hl kwd">lda</span><span class="hl std">(x, y, ...)</span>
 <span class="hl std">}</span>
</pre></div>
</div></div>

<h2>The <span class="mx funCall">pred</span> Function</h2>

<p>
This should be a function that produces predictors for new samples.
</p>
<p>Inputs:
<ul>
 <li> <span class="mx arg">object</span>: the object generated by the <code>fit</code> module. 
   <li> <span class="mx arg">x</span>: a matrix or data frame of predictor data.
</ul>
</p>
<p>
The output is either a number vector (for
regression), a factor (or character) vector for classification or a
matrix/data frame of class probabilities. For classification, it is probably better to average class probabilities instead of using the votes of the class predictions. Using the <span class="mx funCall">lda</span> example again:
</p>
  
<div class="chunk" id="unnamed-chunk-4"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl com">## predict.lda returns the class and the class probabilities We will average</span>
<span class="hl com">## the probabilities, so these are saved</span>
<span class="hl kwa">function</span><span class="hl std">(</span><span class="hl kwc">object</span><span class="hl std">,</span> <span class="hl kwc">x</span><span class="hl std">)</span> <span class="hl kwd">predict</span><span class="hl std">(object, x)</span><span class="hl opt">$</span><span class="hl std">posterior</span>
</pre></div>
</div></div>

    
<h2>The <span class="mx funCall">aggregate</span> Function</h2>

<p>
This should be a function that takes the predictions from the constituent models and converts them to a single prediction per sample.
</p>
<p>Inputs:
<ul>
   <li> <span class="mx arg">x</span>: a list of objects returned by the <code>pred</code> module. 
   <li> <span class="mx arg">type</span>: an optional string that describes the type of output (e.g. "class", "prob" etc.).
</ul>
</p>
<p>
The output is either a number vector (for
regression), a factor (or character) vector for classification or a
matrix/data frame of class probabilities. For the linear discriminant model above, we saved the matrix of class probabilities. To average them and generate a class prediction, we could use:
</p>
<div class="chunk" id="unnamed-chunk-5"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwa">function</span><span class="hl std">(</span><span class="hl kwc">x</span><span class="hl std">,</span> <span class="hl kwc">type</span> <span class="hl std">=</span> <span class="hl str">&quot;class&quot;</span><span class="hl std">) {</span>
  <span class="hl com">## The class probabilities come in as a list of matrices</span>
  <span class="hl com">## For each class, we can pool them then average over them</span>
  <span class="hl com">## Pre-allocate space for the results</span>
  <span class="hl std">pooled</span> <span class="hl kwb">&lt;-</span> <span class="hl std">x[[</span><span class="hl num">1</span><span class="hl std">]]</span> <span class="hl opt">*</span> <span class="hl num">NA</span>
  <span class="hl std">n</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">nrow</span><span class="hl std">(pooled)</span>
  <span class="hl std">classes</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">colnames</span><span class="hl std">(pooled)</span>
  <span class="hl com">## For each class probability, take the median across </span>
  <span class="hl com">## all the bagged model predictions</span>
  <span class="hl kwa">for</span><span class="hl std">(i</span> <span class="hl kwa">in</span> <span class="hl num">1</span><span class="hl opt">:</span><span class="hl kwd">ncol</span><span class="hl std">(pooled))</span>
  <span class="hl std">{</span>
    <span class="hl std">tmp</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">lapply</span><span class="hl std">(x,</span> <span class="hl kwa">function</span><span class="hl std">(</span><span class="hl kwc">y</span><span class="hl std">,</span> <span class="hl kwc">col</span><span class="hl std">) y[,col],</span> <span class="hl kwc">col</span> <span class="hl std">= i)</span>
    <span class="hl std">tmp</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">do.call</span><span class="hl std">(</span><span class="hl str">&quot;rbind&quot;</span><span class="hl std">, tmp)</span>
    <span class="hl std">pooled[,i]</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">apply</span><span class="hl std">(tmp,</span> <span class="hl num">2</span><span class="hl std">, median)</span>
  <span class="hl std">}</span>
  <span class="hl com">## Re-normalize to make sure they add to 1</span>
  <span class="hl std">pooled</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">apply</span><span class="hl std">(pooled,</span> <span class="hl num">1</span><span class="hl std">,</span> <span class="hl kwa">function</span><span class="hl std">(</span><span class="hl kwc">x</span><span class="hl std">) x</span><span class="hl opt">/</span><span class="hl kwd">sum</span><span class="hl std">(x))</span>
  <span class="hl kwa">if</span><span class="hl std">(n</span> <span class="hl opt">!=</span> <span class="hl kwd">nrow</span><span class="hl std">(pooled)) pooled</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">t</span><span class="hl std">(pooled)</span>
  <span class="hl kwa">if</span><span class="hl std">(type</span> <span class="hl opt">==</span> <span class="hl str">&quot;class&quot;</span><span class="hl std">)</span>
  <span class="hl std">{</span>
    <span class="hl std">out</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">factor</span><span class="hl std">(classes[</span><span class="hl kwd">apply</span><span class="hl std">(pooled,</span> <span class="hl num">1</span><span class="hl std">, which.max)],</span>
                  <span class="hl kwc">levels</span> <span class="hl std">= classes)</span>
  <span class="hl std">}</span> <span class="hl kwa">else</span> <span class="hl std">out</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">as.data.frame</span><span class="hl std">(pooled)</span>
  <span class="hl std">out</span>
<span class="hl std">}</span>
</pre></div>
</div></div>

<p>For example, to bag a conditional inference tree (from the <strong>party</strong> package):</p>

<div class="chunk" id="unnamed-chunk-6"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">library</span><span class="hl std">(caret)</span>
<span class="hl kwd">set.seed</span><span class="hl std">(</span><span class="hl num">998</span><span class="hl std">)</span>
<span class="hl std">inTraining</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">createDataPartition</span><span class="hl std">(Sonar</span><span class="hl opt">$</span><span class="hl std">Class,</span> <span class="hl kwc">p</span> <span class="hl std">=</span> <span class="hl num">.75</span><span class="hl std">,</span> <span class="hl kwc">list</span> <span class="hl std">=</span> <span class="hl num">FALSE</span><span class="hl std">)</span>
<span class="hl std">training</span> <span class="hl kwb">&lt;-</span> <span class="hl std">Sonar[ inTraining,]</span>
<span class="hl std">testing</span>  <span class="hl kwb">&lt;-</span> <span class="hl std">Sonar[</span><span class="hl opt">-</span><span class="hl std">inTraining,]</span>
<span class="hl kwd">set.seed</span><span class="hl std">(</span><span class="hl num">825</span><span class="hl std">)</span>
<span class="hl std">baggedCT</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">bag</span><span class="hl std">(</span><span class="hl kwc">x</span> <span class="hl std">= training[,</span> <span class="hl kwd">names</span><span class="hl std">(training)</span> <span class="hl opt">!=</span> <span class="hl str">&quot;Class&quot;</span><span class="hl std">],</span>
                <span class="hl kwc">y</span> <span class="hl std">= training</span><span class="hl opt">$</span><span class="hl std">Class,</span>
                <span class="hl kwc">B</span> <span class="hl std">=</span> <span class="hl num">50</span><span class="hl std">,</span>
                <span class="hl kwc">bagControl</span> <span class="hl std">=</span> <span class="hl kwd">bagControl</span><span class="hl std">(</span><span class="hl kwc">fit</span> <span class="hl std">= ctreeBag</span><span class="hl opt">$</span><span class="hl std">fit,</span>
                                        <span class="hl kwc">predict</span> <span class="hl std">= ctreeBag</span><span class="hl opt">$</span><span class="hl std">pred,</span>
                                        <span class="hl kwc">aggregate</span> <span class="hl std">= ctreeBag</span><span class="hl opt">$</span><span class="hl std">aggregate))</span>
<span class="hl kwd">summary</span><span class="hl std">(baggedCT)</span>
</pre></div>
<div class="output"><pre class="knitr r">
Call:
bag.default(x = training[, names(training) != "Class"], y
 = training$Class, B = 50, bagControl = bagControl(fit =
 ctreeBag$fit, predict = ctreeBag$pred, aggregate = ctreeBag$aggregate))

Out of bag statistics (B = 50):

       Accuracy  Kappa
  0.0%    0.500 0.0713
  2.5%    0.578 0.1544
 25.0%    0.656 0.3172
 50.0%    0.698 0.3919
 75.0%    0.740 0.4738
 97.5%    0.800 0.6004
100.0%    0.867 0.7101
</pre></div>
</div></div>


<div id="avnnet"></div>  
<h1>Model Averaged Neural Networks</h1>

<p>
The <span class="mx funCall">avNNet</span> fits multiple neural network models to the same data set and predicts using the average of the predictions coming from each constituent model. The models can be different either due to different random number seeds to initialize the network or by fitting the models on bootstrap samples of the original training set (i.e. bagging the neural network). For classification models, the class probabilities are averaged to produce the final class prediction (as opposed to voting from the individual class predictions. 
</p>
<p>As an example, the model can be fit via <span class="mx funCall">train</span>:</p>
<div class="chunk" id="unnamed-chunk-7"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">set.seed</span><span class="hl std">(</span><span class="hl num">825</span><span class="hl std">)</span>
<span class="hl std">avNnetFit</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">train</span><span class="hl std">(</span><span class="hl kwc">x</span> <span class="hl std">= training,</span>
                   <span class="hl kwc">y</span> <span class="hl std">= trainClass,</span>
                   <span class="hl kwc">method</span> <span class="hl std">=</span> <span class="hl str">&quot;avNNet&quot;</span><span class="hl std">,</span>
                   <span class="hl kwc">repeats</span> <span class="hl std">=</span> <span class="hl num">15</span><span class="hl std">,</span>
                   <span class="hl kwc">trace</span> <span class="hl std">=</span> <span class="hl num">FALSE</span><span class="hl std">)</span>
</pre></div>
</div></div>

    
<div id="pcannet"></div>      
<h1>Neural Networks with a Principal Component Step</h1>

<p>
Neural networks can be affected by severe amounts of multicollinearity
in the predictors. The function <span class="mx funCall">pcaNNet</span> is a wrapper around
the <span class="mx funCall">preProcess</span> and <span class="mx funCall">nnet</span> functions that will run
principal component analysis on the predictors before using them as
inputs into a neural network. The function will keep enough components
that will capture some pre-defined threshold on the cumulative
proportion of variance (see the <span class="mx arg">thresh</span>
argument). For new samples, the same transformation is applied to the new predictor values (based on the loadings from the training set). The function is available for both regression and classification. 
</p>
<p>
This function is deprecated in favor of the <span class="mx funCall">train</span> function using <span class="mx arg">method</span> = <span class="hl str">&quot;nnet&quot;</span>  and <span class="mx arg">preProc</span> = <span class="hl str">&quot;pca&quot;</span></code>.
</p>

<div id="ica"></div>  
<h1>Independent Component Regression</h1>

<p>
The <span class="mx funCall">icr</span> function can be used to fit a model analogous to
principal component regression (PCR), but using independent component
analysis (ICA). The predictor data are centered and projected to the
ICA components. These components are then regressed against the
outcome. The user needed to specify the number of components to keep.
</p>
<p>
The model uses the <span class="mx funCall">preProcess</span> function to compute the latent
variables using the  <a href="http://cran.r-project.org/web/packages/fastICA/index.html" <strong>fastICA</strong></a> package.
</p>
<p>
Like PCR, there is no guarantee that there will be a correlation
between the new latent variable and the outcomes.
</p>




<div style="clear: both;">&nbsp;</div>
  </div>
  <!-- end #content -->
<div id="sidebar">
  <ul>
  <li>
  <h2>Links</h2>
  <p><a href="modelList.html"><tt>train</tt> Model List</a></p>
  </li>
  <li>
  <h2>Topics</h2>
  <ul>
          <li><a href="index.html">Main Page</a></li>
  		<li><a href="datasets.html">Data Sets</a></li>
                <li><a href="visualizations.html">Visualizations</a></li>
                <li><a href="preprocess.html">Pre-Processing</a></li>
                <li><a href="splitting.html">Data Splitting</a></li>
                <li><a href="misc.html">Miscellaneous Model Functions</a></li>
                <li><a href="training.html">Model Training and Tuning</a></li>
                <li><a href="modelList.html"><tt>train</tt> Model List</a></li>
                <li><a href="bytag.html"><tt>train</tt> Models By Tag</a></li>
                 <li><a href="similarity.html"><tt>train</tt> Models By Similarity</a></li>
                <li><a href="custom_models.html">Using Custom Models</a></li>
                <li><a href="varimp.html">Variable Importance</a></li>
                <li><a href="featureselection.html">Feature Selection</a></li>
                <li><a href="other.html">Other Functions</a></li>
                <li><a href="parallel.html">Parallel Processing</a></li>
                <li><a href="adaptive.html">Adaptive Resampling</a></li> 
</ul>
  </li>
  </ul>
  </div>
  <!-- end #sidebar -->
<div style="clear: both;">&nbsp;</div>
  </div>
  <div class="container"><img src="images/img03.png" width="1000" height="40" alt="" /></div>
  <!-- end #page -->
</div>
  <div id="footer-content"></div>


  <div id="footer">
  <p>Created on Sat May 31 2014 using caret version 6.0-29 and R version 3.0.3 (2014-03-06).</p>
  </div>
  <!-- end #footer -->
</body>
  </html>
