
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
  <!--
  Design by Free CSS Templates
http://www.freecsstemplates.org
Released for free under a Creative Commons Attribution 2.5 License

Name       : Emerald 
Description: A two-column, fixed-width design with dark color scheme.
Version    : 1.0
Released   : 20120902

-->
  <html xmlns="http://www.w3.org/1999/xhtml">
  <head>
  <meta name="keywords" content="" />
  <meta name="description" content="" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <title>Model Training and Tuning</title>
  <link href='http://fonts.googleapis.com/css?family=Abel' rel='stylesheet' type='text/css'>
  <link href="style.css" rel="stylesheet" type="text/css" media="screen" />
  </head>
  <body>
  <div id="wrapper">
  <div id="header-wrapper" class="container">
  <div id="header" class="container">
  <div id="logo">
  <h1><a href="#">Model Training and Tuning</a></h1>
</div>
  <!--
  <div id="menu">
  <ul>
  <li class="current_page_item"><a href="#">Homepage</a></li>
<li><a href="#">Blog</a></li>
<li><a href="#">Photos</a></li>
<li><a href="#">About</a></li>
<li><a href="#">Contact</a></li>
</ul>
  </div>
  -->
  </div>
  <div><img src="images/img03.png" width="1000" height="40" alt="" /></div>
  </div>
  <!-- end #header -->
<div id="page">
  <div id="content">

<h2>Model Training and Parameter Tuning</h2>

<p>
The <a href="http://cran.r-project.org/web/packages/caret/index.html"><strong>caret</strong></a> package has several functions that attempt to streamline the model building and evaluation process. <p>
</p> 
<p>
<p>
The <code>train</code> function can be used to
  <ul>
  <li> evaluate, using resampling, the effect of model tuning parameters on performance
  <li> choose the "optimal" model across these parameters 
  <li> estimate model performance from a training set
  </ul>
</p> 

<p>
First, a specific model must be chosen. Currently,
144 are available using
<a href="http://cran.r-project.org/web/packages/caret/index.html"><strong>caret</strong></a>; see <a href="modelList.html"><tt>train</tt> Model List</a> 
or <a href="bytag.html"><tt>train</tt> Models By Tag</a> for details. 
On these pages, there are lists of tuning parameters that can
potentially be optimized.  The first step in tuning the model (line
1 in the algorithm above is to choose a set of
parameters to evaluate. For example, if fitting a Partial Least Squares
(PLS) model, the number of PLS components to evaluate must be specified. 
</p>

<p><br><img width = 629 height =234 src="TrainAlgo.png"><br><br></p>

<p>
Once the model and tuning parameter values have been defined, the type
of resampling should be also be specified. Currently, <i>k</i>-fold
cross-validation (once or repeated), 
leave-one-out cross-validation and bootstrap
(simple estimation or the 632 rule) 
resampling methods can be used by <code>train</code>. After resampling,
the process produces a profile of performance measures is available to
guide the user as to which tuning parameter values should be
chosen. By default, the function automatically chooses the tuning
parameters associated with the best value, although different
algorithms can be used (see details below below). 
</p>

<h3>An Example</h3>

<p>The Sonar data are available in the <a href="http://cran.r-project.org/web/packages/mlbench/index.html"><strong>mlbench</strong></a> package. Here, we load the data:

<!-- begin{Schunk} !-->
<!-- begin{Sinput} !-->

<p><xmp class=command>> library(mlbench)</xmp></p>



<p><xmp class=command>> data(Sonar)</xmp></p>



<p><xmp class=command>> str(Sonar)</xmp></p>


<!--\end{Sinput}!-->

<!--\end{Schunk}!-->
<!-- begin{Schunk} !--> 
<pre>'data.frame':	208 obs. of  61 variables:</pre> <pre> $ V1   : num  0.02 0.0453 0.0262 0.01 0.0762 0.0286 0.0317 0.0519 0.0223 0.0164 ...</pre> <pre> $ V2   : num  0.0371 0.0523 0.0582 0.0171 0.0666 0.0453 0.0956 0.0548 0.0375 0.0173 ...</pre> <pre> $ V3   : num  0.0428 0.0843 0.1099 0.0623 0.0481 ...</pre> <pre> $ V4   : num  0.0207 0.0689 0.1083 0.0205 0.0394 ...</pre> <pre> $ V5   : num  0.0954 0.1183 0.0974 0.0205 0.059 ...</pre> <pre> $ V6   : num  0.0986 0.2583 0.228 0.0368 0.0649 ...</pre> <pre> $ V7   : num  0.154 0.216 0.243 0.11 0.121 ...</pre> <pre> $ V8   : num  0.16 0.348 0.377 0.128 0.247 ...</pre> <pre> $ V9   : num  0.3109 0.3337 0.5598 0.0598 0.3564 ...</pre> <pre> $ V10  : num  0.211 0.287 0.619 0.126 0.446 ...</pre> <pre> $ V11  : num  0.1609 0.4918 0.6333 0.0881 0.4152 ...</pre> <pre> $ V12  : num  0.158 0.655 0.706 0.199 0.395 ...</pre> <pre> $ V13  : num  0.2238 0.6919 0.5544 0.0184 0.4256 ...</pre> <pre> $ V14  : num  0.0645 0.7797 0.532 0.2261 0.4135 ...</pre> <pre> $ V15  : num  0.066 0.746 0.648 0.173 0.453 ...</pre> <pre> $ V16  : num  0.227 0.944 0.693 0.213 0.533 ...</pre> <pre> $ V17  : num  0.31 1 0.6759 0.0693 0.7306 ...</pre> <pre> $ V18  : num  0.3 0.887 0.755 0.228 0.619 ...</pre> <pre> $ V19  : num  0.508 0.802 0.893 0.406 0.203 ...</pre> <pre> $ V20  : num  0.48 0.782 0.862 0.397 0.464 ...</pre> <pre> $ V21  : num  0.578 0.521 0.797 0.274 0.415 ...</pre> <pre> $ V22  : num  0.507 0.405 0.674 0.369 0.429 ...</pre> <pre> $ V23  : num  0.433 0.396 0.429 0.556 0.573 ...</pre> <pre> $ V24  : num  0.555 0.391 0.365 0.485 0.54 ...</pre> <pre> $ V25  : num  0.671 0.325 0.533 0.314 0.316 ...</pre> <pre> $ V26  : num  0.641 0.32 0.241 0.533 0.229 ...</pre> <pre> $ V27  : num  0.71 0.327 0.507 0.526 0.7 ...</pre> <pre> $ V28  : num  0.808 0.277 0.853 0.252 1 ...</pre> <pre> $ V29  : num  0.679 0.442 0.604 0.209 0.726 ...</pre> <pre> $ V30  : num  0.386 0.203 0.851 0.356 0.472 ...</pre> <pre> $ V31  : num  0.131 0.379 0.851 0.626 0.51 ...</pre> <pre> $ V32  : num  0.26 0.295 0.504 0.734 0.546 ...</pre> <pre> $ V33  : num  0.512 0.198 0.186 0.612 0.288 ...</pre> <pre> $ V34  : num  0.7547 0.2341 0.2709 0.3497 0.0981 ...</pre> <pre> $ V35  : num  0.854 0.131 0.423 0.395 0.195 ...</pre> <pre> $ V36  : num  0.851 0.418 0.304 0.301 0.418 ...</pre> <pre> $ V37  : num  0.669 0.384 0.612 0.541 0.46 ...</pre> <pre> $ V38  : num  0.61 0.106 0.676 0.881 0.322 ...</pre> <pre> $ V39  : num  0.494 0.184 0.537 0.986 0.283 ...</pre> <pre> $ V40  : num  0.274 0.197 0.472 0.917 0.243 ...</pre> <pre> $ V41  : num  0.051 0.167 0.465 0.612 0.198 ...</pre> <pre> $ V42  : num  0.2834 0.0583 0.2587 0.5006 0.2444 ...</pre> <pre> $ V43  : num  0.282 0.14 0.213 0.321 0.185 ...</pre> <pre> $ V44  : num  0.4256 0.1628 0.2222 0.3202 0.0841 ...</pre> <pre> $ V45  : num  0.2641 0.0621 0.2111 0.4295 0.0692 ...</pre> <pre> $ V46  : num  0.1386 0.0203 0.0176 0.3654 0.0528 ...</pre> <pre> $ V47  : num  0.1051 0.053 0.1348 0.2655 0.0357 ...</pre> <pre> $ V48  : num  0.1343 0.0742 0.0744 0.1576 0.0085 ...</pre> <pre> $ V49  : num  0.0383 0.0409 0.013 0.0681 0.023 0.0264 0.0507 0.0285 0.0777 0.0092 ...</pre> <pre> $ V50  : num  0.0324 0.0061 0.0106 0.0294 0.0046 0.0081 0.0159 0.0178 0.0439 0.0198 ...</pre> <pre> $ V51  : num  0.0232 0.0125 0.0033 0.0241 0.0156 0.0104 0.0195 0.0052 0.0061 0.0118 ...</pre> <pre> $ V52  : num  0.0027 0.0084 0.0232 0.0121 0.0031 0.0045 0.0201 0.0081 0.0145 0.009 ...</pre> <pre> $ V53  : num  0.0065 0.0089 0.0166 0.0036 0.0054 0.0014 0.0248 0.012 0.0128 0.0223 ...</pre> <pre> $ V54  : num  0.0159 0.0048 0.0095 0.015 0.0105 0.0038 0.0131 0.0045 0.0145 0.0179 ...</pre> <pre> $ V55  : num  0.0072 0.0094 0.018 0.0085 0.011 0.0013 0.007 0.0121 0.0058 0.0084 ...</pre> <pre> $ V56  : num  0.0167 0.0191 0.0244 0.0073 0.0015 0.0089 0.0138 0.0097 0.0049 0.0068 ...</pre> <pre> $ V57  : num  0.018 0.014 0.0316 0.005 0.0072 0.0057 0.0092 0.0085 0.0065 0.0032 ...</pre> <pre> $ V58  : num  0.0084 0.0049 0.0164 0.0044 0.0048 0.0027 0.0143 0.0047 0.0093 0.0035 ...</pre> <pre> $ V59  : num  0.009 0.0052 0.0095 0.004 0.0107 0.0051 0.0036 0.0048 0.0059 0.0056 ...</pre> <pre> $ V60  : num  0.0032 0.0044 0.0078 0.0117 0.0094 0.0062 0.0103 0.0053 0.0022 0.004 ...</pre> <pre> $ Class: Factor w/ 2 levels "M","R": 2 2 2 2 2 2 2 2 2 2 ...</pre>
<!--\end{Schunk}!-->
<p>
There are a few factor predictors, so we will decompose these into dummy variables 
using the <code>dummyVars</code> function in this package. This decomposition is applied
to the training and test sets.
</p>
<!-- begin{Schunk} !-->
<!-- begin{Sinput} !-->

<p><xmp class=command>> library(caret)</xmp></p>



<p><xmp class=command>> set.seed(998)</xmp></p>



<p><xmp class=command>> inTraining <- createDataPartition(Sonar$Class, p = 0.75, list = FALSE)</xmp></p>



<p><xmp class=command>> training <- Sonar[inTraining, ]</xmp></p>



<p><xmp class=command>> testing <- Sonar[-inTraining, ]</xmp></p>


<!--\end{Sinput}!-->

<!--\end{Schunk}!-->

<h2>Basic Parameter Tuning</h2>

<p>
By default, simple bootstrap resampling is used for line
3 in the algorithm above. Others are availible, such
as repeated <i>K</i>-fold cross-validation, leave-one-out etc. The function
<code>trainControl</code> can be used to specifiy the type of resampling:
</p>
<p><xmp class=command>> fitControl <- trainControl(## 10-fold CV </xmp></p>
<p><xmp class=command>>                           method = "repeatedcv", </xmp></p>
<p><xmp class=command>>                           number = 10, </xmp></p>
<p><xmp class=command>>                           ## repeated ten times </xmp></p>
<p><xmp class=command>>                           repeats = 10) </xmp></p>
<p>
More information about <code>trainControl</code> is given in the section on "X" below.
</p>

<p>The first two arguments to <code>train</code> are the predictor and
outcome data objects, respectively. The third argument,
<code>method</code>, specifies the type of model (see <a href="modelList.html"><tt>train</tt> Model List</a> 
or <a href="bytag.html"><tt>train</tt> Models By Tag</a>). 
To illustrate, we will fit a boosted tree model via the <a href="http://cran.r-project.org/web/packages/gbm/index.html"><strong>gbm</strong></a>
package. The basic syntax for fitting this model using repeated
cross-validation is shown below:
</p>
<p><xmp class=command>> set.seed(825) </xmp></p>
<p><xmp class=command>> gbmFit1 <- train(Class ~ ., data = training,  </xmp></p>
<p><xmp class=command>>                  method = "gbm",  </xmp></p>
<p><xmp class=command>>                  trControl = fitControl, </xmp></p>
<p><xmp class=command>>                  ## This last option is actually one </xmp></p>
<p><xmp class=command>>                  ## for gbm() that passes through </xmp></p>
<p><xmp class=command>>                  verbose = FALSE) </xmp></p>
<p><xmp class=command>> gbmFit1 </xmp></p>

<!-- begin{Schunk} !--> 
<pre>157 samples</pre> <pre> 60 predictors</pre> <pre>  2 classes: 'M', 'R' </pre> <pre></pre> <pre>No pre-processing</pre> <pre>Resampling: Cross-Validation (10 fold, repeated 10 times) </pre> <pre></pre> <pre>Summary of sample sizes: 142, 142, 140, 142, 142, 141, ... </pre> <pre></pre> <pre>Resampling results across tuning parameters:</pre> <pre></pre> <pre>  interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD</pre> <pre>  1                  50       0.763     0.521  0.104        0.209   </pre> <pre>  1                  100      0.784     0.562  0.102        0.209   </pre> <pre>  1                  150      0.799     0.593  0.101        0.208   </pre> <pre>  2                  50       0.792     0.577  0.0954       0.195   </pre> <pre>  2                  100      0.811     0.617  0.0942       0.193   </pre> <pre>  2                  150      0.817     0.628  0.0867       0.178   </pre> <pre>  3                  50       0.805     0.605  0.0988       0.2     </pre> <pre>  3                  100      0.82      0.635  0.0899       0.184   </pre> <pre>  3                  150      0.826     0.647  0.0796       0.163   </pre> <pre></pre> <pre>Tuning parameter 'shrinkage' was held constant at a value of 0.1</pre> <pre>Accuracy was used to select the optimal model using  the largest value.</pre> <pre>The final values used for the model were interaction.depth = 3, n.trees = 150 and shrinkage = 0.1. </pre>
<!--\end{Schunk}!-->

<p>
For a gradient boosting machine (GBM) model, there are three main
tuning parameters:
  <ul>
  <li> number of iterations, i.e. trees,  (called <code>n.trees</code> in the
  <code>gbm</code> function)
  <li> complexity of the tree, called <code>interaction.depth</code>
  <li> learning rate: how quickly the algorithm adapts, called
  <code>shrinkage</code> 
  </ul>
</p>  
<p>
The default values tested for this model are shown in the first two
columns (<code>shrinkage</code> is not shown beause the grid set of
candidate models all use a value of 0.1 for this tuning parameter). 
The column labeled "<code>Accuracy</code>" is the overall agreement rate
averaged over cross-validation iterations. The agreement standard
deviation is also calculated from the cross-validation results. The
column "<code>Kappa</code>" is Cohen's (unweighted) Kappa statistic
averaged across the resampling results. <code>train</code> works with
specific models (see <a href="modelList.html"><tt>train</tt> Model List</a> 
or <a href="bytag.html"><tt>train</tt> Models By Tag</a>). 
For these models, <code>train</code> can automatically
create a grid of tuning parameters. By default, if <i>p</i> is the number
of tuning parameters, the grid size is <i>3^p</i>. As another example, regularized
discriminant analysis (RDA) models have two  parameters
(<code>gamma</code> and <code>lambda</code>), both of which lie on [0,
1]. The default training grid would produce nine combinations in this
two-dimensional space. 
</p>

<p>There are several <a href="notes.html">notes</a> regarding specific model behaviors 
for <code>train</code>. There is additional functionality in <code>train</code> that is described in the next section.
</p>

<h2>Customizing the Tuning Process</h2>

<p>
There are a few ways to customize the process of selecting
tuning/complexity parameters and building the final model.
</p>

<h3>Pre-Processing Options</h3>

<p>
As previously mentioned,<code>train</code> can pre-process the data in
various ways prior to model fitting. The <code>train</code> function
<code>preProcess</code> is automatically used. This function can be used
for centering and scaling, imputation (see details below),
applying the spatial sign transformation and feature extraction via
principal component analysis or independent component
analysis. Options to the <code>preProcess</code> function can be passed
via the <code>trainControl</code> function.  
</p>
<p>
These processing steps would be applied during any predictions
generated using <code>predict.train</code>, <code>extractPrediction</code> or
<code>extractProbs</code> (see details later in this document). The
pre-processing would <b>not</b> be applied to predictions that
directly use the <code>object$finalModel</code> object.  
</p>
<p>
For imputation, there are two methods currently implemented:
</p>

  <ul>
  <li> <i>k</i>-nearest neighbors takes a sample with missing values and
  finds the $k$ closest samples in the training set. The average of
  the $k$ training set values for that predictor are used as a
  substitute for the original data. When calculating the distances to
  the training set samples, the predictors used in the calculation are
  the ones with no missing values for that sample and no missing
  values in the training set.</li>
  <li> another approach is to fit a bagged tree model for each
  predictor using the training set samples. This is usually a fairly
  accurate model and can handle missing values. When a predictor for a
  sample requires imputation, the values for the other predictors are
  fed through the bagged tree and the prediction is used as the new
  value. This model can have significant computational cost.</li>
  </ul>
<p>
If there are missing values in the training set, PCA and ICA models
only use complete samples.
</p>

<h3>Alternate Tuning Grids</h3>
<p>
The tuning parameter grid can be specified by the user. The argument
<code>tuneGrid</code> can take a data frame with columns for each tuning
parameter. The column
names should be the same as the fitting function's arguments with a
period preceding the name. For the previously mentioned RDA example, the names would be
<code>.gamma</code> and <code>.lambda</code>. <code>train</code> will tune the
model over each combination of values in the rows. 
</p>
<p>
We can fix the learning rate and evaluate more than three values of
<code>n.trees</code>: 
</p>

<p><xmp class=command>> gbmGrid <-  expand.grid(.interaction.depth = c(1, 5, 9),  </xmp></p>
<p><xmp class=command>>                         .n.trees = (1:15)*100,  </xmp></p>
<p><xmp class=command>>                         .shrinkage = 0.1) </xmp></p>
<p><xmp class=command>> nrow(gbmGrid) </xmp></p>

<!-- begin{Schunk} !--> 
<pre>[1] 45</pre>
<!--\end{Schunk}!-->

<p><xmp class=command>> set.seed(825)</xmp></p>
<p><xmp class=command>> gbmFit2 <- train(Class ~ ., data = training, </xmp></p>
<p><xmp class=command>>                  method = "gbm", </xmp></p>
<p><xmp class=command>>                  trControl = fitControl,</xmp></p> 
<p><xmp class=command>>                  verbose = FALSE, </xmp></p>
<p><xmp class=command>>                  ## Now specify the exact models </xmp></p>
<p><xmp class=command>>                  ## to evaludate:</xmp></p>
<p><xmp class=command>>                  tuneGrid = gbmGrid)</xmp></p>
<p><xmp class=command>> gbmFit2</xmp></p>
<!-- begin{Schunk} !--> 
<pre> 157 samples </pre> <pre>  60 predictors </pre> <pre>   2 classes: 'M', 'R'  </pre> <pre>  </pre> <pre> No pre-processing </pre> <pre> Resampling: Cross-Validation (10 fold, repeated 10 times)  </pre> <pre>  </pre> <pre> Summary of sample sizes: 142, 142, 140, 142, 142, 141, ...  </pre> <pre>  </pre> <pre> Resampling results across tuning parameters: </pre> <pre>  </pre> <pre>   interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD </pre> <pre>   1                  100      0.795     0.585  0.0915       0.187    </pre> <pre>   1                  200      0.804     0.602  0.0899       0.184    </pre> <pre>   1                  300      0.804     0.603  0.0909       0.186    </pre> <pre>   1                  400      0.809     0.612  0.0866       0.177    </pre> <pre>   1                  500      0.811     0.618  0.0838       0.171    </pre> <pre>   1                  600      0.811     0.618  0.0884       0.18     </pre><pre>   :                  :         :         :       :            :</pre>
<pre>   9                  900      0.828     0.649  0.0951       0.195    </pre>
<pre>   9                  1000     0.826     0.647  0.0933       0.191    </pre>
<pre>   9                  1100     0.827     0.648  0.0947       0.194    </pre>
<pre>   9                  1200     0.824     0.641  0.0927       0.19     </pre>
<pre>   9                  1300     0.823     0.64   0.0913       0.187    </pre>
<pre>   9                  1400     0.82      0.636  0.095        0.194    </pre>
<pre>   9                  1500     0.822     0.64   0.0926       0.188    </pre>
<pre>  </pre>
<pre> Tuning parameter 'shrinkage' was held constant at a value of 0.1 </pre>
<pre> Accuracy was used to select the optimal model using  the largest value. </pre>
<pre> The final values used for the model were interaction.depth = 9, n.trees = 600 and shrinkage = 0.1.  </pre>
<!--\end{Schunk}!-->

<h3>Plotting the Resampling Profile</h3>

<p>The <code>plot</code> function can be used to examine the relationship between the 
estimates of performance and the tuning parameters. For example, a simple invokation of 
the function shows the results for the first performance measure:
</p>
<p><xmp class=command>> plot(gbmFit2) </xmp></p>
<p>produces the following plot:
</p>

<!-- begin{Schunk} !--> 
<p><br><a href="Train_Plot1.pdf"><img width =300 height =210 src="Train_Plot1.png"><br>(click image for larger pdf)</a><br><br></p>
<!--\end{Schunk}!-->
<p>Other performance metrics can be shown using the <code>metric</code> option:
</p>
<p><xmp class=command>> plot(gbmFit2, metric = "Kappa") </xmp></p>
<p>produces the following plot:
</p>

<!-- begin{Schunk} !--> 
<p><br><a href="Train_Plot2.pdf"><img width =300 height =210 src="Train_Plot2.png"><br>(click image for larger pdf)</a><br><br></p>
<!--\end{Schunk}!-->
<p>Other types of plot are also available. See <code>?plot.train</code> for more details.
The code below shows a heatmap of the results:
</p>
<p><xmp class=command>> plot(gbmFit2, metric = "Kappa",</xmp></p>
<p><xmp class=command>>       plotType = "level",</xmp></p>
<p><xmp class=command>>       scales = list(x = list(rot = 90)))</xmp></p>
<p>produces the following plot:
</p>

<!-- begin{Schunk} !--> 
<p><br><a href="Train_Plot3.pdf"><img width =300 height =210 src="Train_Plot3.png"><br>(click image for larger pdf)</a><br><br></p>
<!--\end{Schunk}!-->
<p>There are also plot functions that show more detailed representations of the 
resampled estimates. See <code>?xyplot.train</code> for more details.
</p>
<p>
From these plots, a different set of tuning parameters may be desired. To change the 
final values without starting the whole process again, the <code>update.train</code>
can be used to refit the final model. See <code>?update.train</code>
</p>

<h3>The <code>trainControl</code> Function</h3>

<p>
The function <code>trainControl</code> generates parameters that further
control how models are created, with possible values: 
</p>
<ul>
 <li> <code>method</code>: The resampling method: <code>boot</code>, <code>boot632</code>,
  <code>cv</code>, <code>LOOCV</code>, <code>LGOCV</code>, 
  <code>repeatedcv</code>  and <code>oob</code>. The
  last value, out-of-bag estimates, can only be used by random
  forest, bagged trees, bagged earth, bagged flexible discriminant
  analysis, or conditional tree forest models. GBM models are not
  included (the <a href="http://cran.r-project.org/web/packages/gbm/index.html"><strong>gbm</strong></a> package maintainer has indicated that
  it would not be a good idea to choose tuning parameter values
  based on the model OOB error estimates with boosted trees). Also,
  for leave-one-out cross-validation, no uncertainty estimates
  are given for the resampled performance measures.  </li>
 <li> <code>number</code> and <code>repeats</code>: <code>number</code> controls with the 
  number of folds in <i>K</i>-fold cross-validation or number of
  resampling iterations for bootstrapping and leave-group-out
  cross-validation. <code>repeats</code> applied only to repeated  
  <i>K</i>-fold cross-validation. Suppose that <code>method = "repeatedcv"</code>,
  <code>number = 10</code> and <code>repeats = 3</code>,then three separate
  10-fold cross-validations are used as the resampling scheme. </li>
 <li> <code>verboseIter</code>: A logical for printing a training log. </li>
 <li> <code>returnData</code>: A logical for saving the data into a slot
  called <code>trainingData</code>.  </li>
 <li> <code>p</code>: For leave-group out cross-validation: the training
  percentage  </li>
 <li> <code>classProbs</code>: a logical value determining whether
  class probabilities should be computed for held-out samples
  during resample.  </li>
 <li> <code>index</code>: a list with elements for each resampling
  iteration. Each list element is the sample rows used for training
  at that iteration. When these values are not specified,
  <code>train</code> will generate them.  </li>
 <li> <code>summaryFunction</code>: a function to compute alternate
  performance summaries. 
  details. </li> 
 <li> <code>selectionFunction</code>: a function to choose the optimal
  tuning parameters. 
  and examples.  </li>
 <li> <code>PCAthresh</code>, <code>ICAcomp</code> and <code>k</code>: these are
  all options to pass to the <code>preProcess</code>function (when used). </li>
 <li> <code>returnResamp</code>: a character string containing one of
  the following values: <code>"all"</code>, <code>"final"</code> or
  <code>"none"</code>. This specifies how much of the resampled
  performance measures to save. </li>  
</ul>

<h3>Alternate Performance Metrics</h3>

<p>
The user can change the metric used to determine the best settings. By
default, RMSE and <i>R</i><sup>2</sup> are computed for regression while accuracy and
Kappa are computed for classification. Also by default, the parameter
values are chosen using RMSE and accuracy, respectively  for
regression and classification. The <code>metric</code> argument of the
<code>train</code> function allows the user to control which the
optimality criterion is used. For example, in problems where there are
a low percentage of samples in one class, using <code>metric =
  "Kappa"</code> can improve quality of the final model. 
</p>
<p>
If none of these parameters are satisfactory, the user can also
compute custom performance metrics. The <code>trainControl</code> function
has a argument called <code>summaryFunction</code> that specifies a
function for computing performance. The function should have these
arguments: 
</p>
<p>
<ul>
 <li> <code>data</code> is a reference for a data frame or matrix with
  columns called <code>obs</code> and <code>pred</code> for the observed and
  predicted outcome values (either numeric data for regression or
  character values for classification). Currently, class probabilities
  are not passed to the function. The values in data are the held-out
  predictions (and their associated reference values) for a single
  combination of tuning parameters. If the <code>classProbs</code>
  argument of the <code>trainControl</code> object is set to
  <code>TRUE</code>, additional columns in <code>data</code> will be present
  that contains the class probabilities. The names of these columns
  are the same as the class levels.</li>
 <li> <code>lev</code> is a character string that has the outcome factor
  levels taken from the training data. For regression, a value of
  <code>NULL</code> is passed into the function. </li>
 <li> <code>model</code> is a character string for the model being used
  (i.e. the value passed to the <code>method</code> argument of
  <code>train</code>). </li>
</ul>
</p>
<p>
The output to the function should be a vector of numeric summary
metrics with non-null names.  By default, <code>train</code> evaluate classification models in terms of
the predicted classes. Optionally, class probabilities can also be
used to measure performance. To obtain predicted class probabilities
within the resampling process, the argument <code>classProbs</code> in
<code>trainControl</code> must be set to <code>TRUE</code>. This merges
columns of probabilities into the predictions generated from each
resample (there is a column per class and the column names are the
class names).
</p>
<p>
As shown in the last section, custom functions can be used to
calculate performance scores that are averaged over the
resamples. Another built-in function, <code>twoClassSummary</code>, will
compute the sensitivity, specificity and area under the ROC curve:
</p>
<!-- begin{Schunk} !-->
<!-- begin{Sinput} !-->

<p><xmp class=command>> head(twoClassSummary)</xmp></p>


<!--\end{Sinput}!-->

<!--\end{Schunk}!-->
<!-- begin{Schunk} !--> 
<pre>                                                                           </pre> <pre>1 function (data, lev = NULL, model = NULL)                                </pre> <pre>2 {                                                                        </pre> <pre>3     require(pROC)                                                        </pre> <pre>4     if (!all(levels(data[, "pred"]) == levels(data[, "obs"])))           </pre> <pre>5         stop("levels of observed and predicted data do not match")       </pre> <pre>6     rocObject <- try(pROC:::roc(data$obs, data[, lev[1]]), silent = TRUE)</pre>
<!--\end{Schunk}!-->

<p>
To rebuild the boosted tree model using this criterion, we
can see the relationship between the tuning parameters and
the area under the ROC curve using the following code: 
</p>

<p><xmp class=command>> fitControl <- trainControl(method = "repeatedcv",</xmp></p>
<p><xmp class=command>>                            number = 10,</xmp></p>
<p><xmp class=command>>                            repeats = 10,</xmp></p>
<p><xmp class=command>>                            ## Estimate class probabilities</xmp></p>
<p><xmp class=command>>                            classProbs = TRUE,</xmp></p>
<p><xmp class=command>>                            ## Evaluate performance using </xmp></p>
<p><xmp class=command>>                            ## the following function</xmp></p>
<p><xmp class=command>>                            summaryFunction = twoClassSummary)   </xmp></p>                  
<p><xmp class=command>> set.seed(825)</xmp></p>
<p><xmp class=command>> gbmFit3 <- train(Class ~ ., data = training, </xmp></p>
<p><xmp class=command>>                  method = "gbm", </xmp></p>
<p><xmp class=command>>                  trControl = fitControl, </xmp></p>
<p><xmp class=command>>                  verbose = FALSE, </xmp></p>
<p><xmp class=command>>                  tuneGrid = gbmGrid,</xmp></p>
<p><xmp class=command>>                  ## Specify which metric to optimize</xmp></p>
<p><xmp class=command>>                  metric = "ROC")</xmp></p>
<p><xmp class=command>> gbmFit3</xmp></p>

<!-- begin{Schunk} !--> 
<pre> 157 samples </pre> <pre>  60 predictors </pre> <pre>   2 classes: 'M', 'R'  </pre> <pre>  </pre> <pre> No pre-processing </pre> <pre> Resampling: Cross-Validation (10 fold, repeated 10 times)  </pre> <pre>  </pre> <pre> Summary of sample sizes: 142, 142, 140, 142, 142, 141, ...  </pre> <pre>  </pre> <pre> Resampling results across tuning parameters: </pre> <pre>  </pre> <pre>   interaction.depth  n.trees  ROC    Sens   Spec   ROC SD  Sens SD  Spec SD </pre> <pre>   1                  100      0.878  0.831  0.719  0.0881  0.127    0.165   </pre> <pre>   1                  200      0.884  0.86   0.741  0.0858  0.116    0.16    </pre> <pre>   1                  300      0.884  0.863  0.739  0.0856  0.118    0.15    </pre> <pre>   1                  400      0.885  0.858  0.751  0.0864  0.111    0.159   </pre> <pre>   1                  500      0.883  0.859  0.748  0.0878  0.109    0.159   </pre> <pre>   1                  600      0.883  0.86   0.751  0.0859  0.112    0.156   </pre><pre>   :                   :         :       :      :     :       :        :</pre>
<pre>   9                  900      0.907  0.891  0.759  0.0827  0.116    0.155   </pre>
<pre>   9                  1000     0.906  0.887  0.763  0.0831  0.114    0.152   </pre>
<pre>   9                  1100     0.902  0.881  0.756  0.102   0.114    0.155   </pre>
<pre>   9                  1200     0.901  0.887  0.759  0.103   0.117    0.15    </pre>
<pre>   9                  1300     0.902  0.885  0.761  0.102   0.118    0.15    </pre>
<pre>   9                  1400     0.903  0.873  0.774  0.102   0.127    0.156   </pre>
<pre>   9                  1500     0.903  0.869  0.791  0.104   0.131    0.148   </pre>
<pre>  </pre>
<pre> Tuning parameter 'shrinkage' was held constant at a value of 0.1 </pre>
<pre> ROC was used to select the optimal model using  the largest value. </pre>
<pre> The final values used for the model were interaction.depth = 5, n.trees = 900 and shrinkage = 0.1.  </pre>
<!--\end{Schunk}!-->

<p>
In this case, the average area under the ROC curve associated with the
optimal tuning parameters was
0.913 across
the 100 resamples.
</p>


<h3>Choosing the Final Model</h3>

<p>
Another method for customizing the tuning process is to modify the
algorithm that is used to select the "best" parameter values, given
the performance numbers. By default, the <code>train</code> function
chooses the model with the largest performance value (or smallest, for
mean squared error in regression models). Other schemes for selecting
model can be used.  <a href="http://books.google.com/books/about/Classification_and_Regression_Trees.html?id=JwQx-WOmSyQC">Breiman et al (1984)</a> suggested the "one standard
error rule" for simple tree-based models. In this case, the model
with the best performance value is identified and, using resampling,
we can estimate the standard error of performance. The final model
used was the simplest model within one standard error of the
(empirically) best model. With simple trees this makes sense, since
these models will start to over-fit as they become more and more
specific to the training data. 
</p>
<p>
<code>train</code> allows the user to specify alternate rules for
selecting the final model. The argument <code>selectionFunction</code>
can be used to supply a function to algorithmically determine the
final model. There are three existing functions in the package:
<code>best</code> is chooses the largest/smallest value, <code>oneSE</code>
attempts to capture the spirit of <a href="http://books.google.com/books/about/Classification_and_Regression_Trees.html?id=JwQx-WOmSyQC">Breiman et al (1984)</a> and
<code>tolerance</code> selects the least complex model within some percent
tolerance of the best value. See <code>?best</code> for more details. 
</p>
<p>
User-defined functions can be used, as long as they have the
following arguments: 
<ul>
  <li><code>x</code> is a data frame containing the tune parameters and
    their associated performance metrics. Each row corresponds to a
    different tuning parameter combination. </li> 
  <li><code>metric</code> a character string indicating which
      performance metric should be optimized (this is passed in
      directly from the <code>metric</code> argument of <code>train</code>.</li>  
  <li><code>maximize</code> is a single logical value indicating
        whether larger values of the performance metric are better
        (this is also directly passed from the call to
        <code>train</code>). </li>
</ul>
<p>
  The function should output a single integer indicating which row in
  <code>x</code> is chosen. 
</p>
  
<p>  
As an example, if we chose the previous boosted tree model on the basis of
overall accuracy, we would choose:
 interaction depth = 5,  n trees = 900,  shrinkage = 0.1. However, the scale in this plots is
fairly tight, with accuracy values ranging from
0.878 to
0.913. A less complex model
(e.g. fewer, more shallow trees) might also yield acceptable
accuracy. 
</p>
<p>
The tolerance function could be used to find a less complex
model based on (<i>x</i>-<i>x</i><sub>best</sub>)/<i>x</i><sub>best</sub>x 100$, which is the percent
difference. For example, to select parameter values based on a 2% loss of performance: 
</p>
<!-- begin{Schunk} !-->
<!-- begin{Sinput} !-->

<p><xmp class=command>> whichTwoPct <- tolerance(gbmFit3$results, "ROC", 2, TRUE)</xmp></p>



<p><xmp class=command>> cat("best model within 2 pct of best:\n")</xmp></p>



<p><xmp class=command>> gbmFit3$results[whichTwoPct, ]</xmp></p>


<!--\end{Sinput}!-->

<!--\end{Schunk}!-->
<!-- begin{Schunk} !--> 
<pre>   interaction.depth n.trees shrinkage       ROC      Sens      Spec      ROCSD   SensSD    SpecSD</pre> <pre>16                 5     100       0.1 0.9037029 0.8633333 0.7721429 0.07035011 0.118736 0.1621036</pre>
<!--\end{Schunk}!-->
<p>
This indicates that we can get a less complex model with and accuracy
of 0.904 (compared
to the "pick the best" value of 
 0.913). 
</p>
<p>
The main issue with these functions is related to ordering the models
from simplest to complex. In some cases, this is easy (e.g. simple
trees, partial least squares), but in cases such as this model, the ordering of
models is subjective. For example, is a boosted tree model using 100
iterations and a tree depth of 2 more complex than one with 50
iterations and a depth of 8? The package makes some choices regarding
the orderings. In the case of boosted trees, the package assumes that
increasing the number of iterations adds complexity at a faster rate
than increasing the tree depth, so models are ordered on the number of
iterations then ordered with depth. See <code>?best</code> for more
examples for specific models. 
</p>

<h2>Extracting Predictions and Class Probabilities</h2>

<p>
As previously mentioned, objects produced by the <code>train</code>
function contain the "optimized" model in the <code>finalModel</code>
sub-object. Predictions can be made from these objects as usual. In
some cases, such as <code>pls</code> or <code>gbm</code> objects, additional
parameters from the optimized fit may need to be specified. In these
cases, the <code>train</code> objects uses the results of the parameter
optimization to predict new samples. For example, if predictions were create
using <code>predict.gbm</code>, the user would have to specify the number of trees 
directly (there is no default). Also, for binary classification, the predictions
from this function take the form of the probability of one of the classes, so 
extra steps are required to convert this to a factor vector. <code>predict.train</code>
automatically handles these details for this (and for other models).
</p>
<p>
Also, there are very few standard syntaxes for model predictions in R. For example, 
to get class probabilities, many <code>predict</code> methods have an argument called 
<code>type</code> that is used to specify whether the classes or probabilities should 
be generated. Different packages use different values of <code>type</code>, such as 
"prob", "posterior", "response", "probability" or "raw". In other cases, completely 
different syntax is used.
</p>
<p>For <code>predict.train</code>, the type options are standardized to be "class" 
and "prob" (the underlying code matches these to the appropriate choices for each 
model. For example:
</p>
<!-- begin{Schunk} !-->
<!-- begin{Sinput} !-->

<p><xmp class=command>> predict(gbmFit3, head(testing))</xmp></p>


<!--\end{Sinput}!-->

<!--\end{Schunk}!-->
<!-- begin{Schunk} !--> 
<pre>[1] R R R R M M</pre> <pre>Levels: M R</pre>
<!--\end{Schunk}!-->
<!-- begin{Schunk} !-->
<!-- begin{Sinput} !-->

<p><xmp class=command>> predict(gbmFit3, head(testing), type = "prob")</xmp></p>


<!--\end{Sinput}!-->

<!--\end{Schunk}!-->
<!-- begin{Schunk} !--> 
<pre>             M            R</pre> <pre>1 2.571188e-07 9.999997e-01</pre> <pre>2 5.284334e-08 9.999999e-01</pre> <pre>3 1.441567e-17 1.000000e+00</pre> <pre>4 1.056927e-06 9.999989e-01</pre> <pre>5 9.999927e-01 7.301546e-06</pre> <pre>6 9.978181e-01 2.181921e-03</pre>
<!--\end{Schunk}!-->


<h2>Exploring and Comparing Resampling Distributions</h2>

<h3>Within-Model</h3>
<p>resamples = "all" note</p>
<p>
There are several <a href="http://cran.r-project.org/web/packages/lattice/index.html"><strong>lattice</strong></a> functions than can be used to explore
relationships between tuning parameters and the resampling results for
a specific model:
<ul>
 <li><code>xyplot</code> and <code>stripplot</code> can be used to plot resampling statistics
  against (numeric) tuning parameters.</li>
 <li><code>histogram</code> and <code>densityplot</code> can also be used
      to look at distributions of the tuning parameters across tuning parameters.</li>
</ul>
</p>
<p>
For example, the following statements   
</p>
<p><xmp class=command>> densityplot(gbmFit3, pch = "|")</xmp></p>
<p>produce the following plot:
</p>

<!-- begin{Schunk} !--> 
<p><br><a href="Train_Dens.pdf"><img width =300 height =150 src="Train_Dens.png"><br>(click image for larger pdf)</a><br><br></p>
<!--\end{Schunk}!-->

<h3>Between-Models</h3>

<p>
The <a href="http://cran.r-project.org/web/packages/caret/index.html"><strong>caret</strong></a> package also includes functions to characterize the differences
between models (generated using <code>train</code>, <code>sbf</code> or
<code>rfe</code>) via their resampling distributions. These functions are
based on the work of <a href="http://www.stat.uni-muenchen.de/~leisch/papers/Hothorn+Leisch+Zeileis-2005.pdf">Hothorn et al. (2005)</a> and <a href="http://epub.ub.uni-muenchen.de/10604/1/tr56.pdf">Eugster et al (2008)</a>.
</p>
<p>
First, a support vector machine model is fit to the Sonar data. The data are centered and scaled using hte <code>preProc</code> argument. Note that the same random number seed is set prior to the model that is idenditcal to the seed used for the boosted tree model. This ensures that the same resampling sets are used, which will come in handy when we compare the resamling profiles between models. </p>
</p>
<p><xmp class=command>> set.seed(825)</xmp></p>
<p><xmp class=command>> svmFit <- train(Class ~ ., data = training, </xmp></p>
<p><xmp class=command>>                  method = "svmRadial", </xmp></p>
<p><xmp class=command>>                  trControl = fitControl, </xmp></p>
<p><xmp class=command>>                  preProc = c("center", "scale"),</xmp></p>
<p><xmp class=command>>                  tuneLength = 8,</xmp></p>
<p><xmp class=command>>                  metric = "ROC")</xmp></p>
<p><xmp class=command>> svmFit</xmp></p>

<!-- begin{Schunk} !--> 
<pre>157 samples</pre> <pre> 60 predictors</pre> <pre>  2 classes: 'M', 'R' </pre> <pre></pre> <pre>Pre-processing: centered, scaled </pre> <pre>Resampling: Cross-Validation (10 fold, repeated 10 times) </pre> <pre></pre> <pre>Summary of sample sizes: 142, 142, 140, 142, 142, 141, ... </pre> <pre></pre> <pre>Resampling results across tuning parameters:</pre> <pre></pre> <pre>  C     ROC    Sens   Spec   ROC SD  Sens SD  Spec SD</pre> <pre>  0.25  0.874  0.952  0.546  0.0831  0.0788   0.18   </pre> <pre>  0.5   0.912  0.919  0.665  0.062   0.0973   0.176  </pre> <pre>  1     0.937  0.908  0.744  0.0539  0.102    0.165  </pre> <pre>  2     0.943  0.899  0.764  0.0519  0.107    0.156  </pre> <pre>  4     0.953  0.922  0.793  0.0502  0.0976   0.156  </pre> <pre>  8     0.952  0.93   0.788  0.0493  0.0905   0.158  </pre> <pre>  16    0.952  0.93   0.788  0.0493  0.0905   0.158  </pre> <pre>  32    0.952  0.93   0.788  0.0493  0.0905   0.158  </pre> <pre></pre> <pre>Tuning parameter 'sigma' was held constant at a value of 0.0168</pre> <pre>ROC was used to select the optimal model using  the largest value.</pre> <pre>The final values used for the model were C = 4 and sigma = 0.0168. </pre>
<!--\end{Schunk}!-->
<p>
Also, a regularized discriminant analysis model was fit.  
</p>
<!-- begin{Schunk} !-->
<!-- begin{Sinput} !-->

<p><xmp class=command>> set.seed(825)</xmp></p>



<p><xmp class=command>> rdaFit <- train(Class ~ ., data = training, method = "rda", trControl = fitControl, </xmp></p>

<p><xmp class=command>>     tuneLength = 4, metric = "ROC")</xmp></p>



<p><xmp class=command>> rdaFit</xmp></p>


<!--\end{Sinput}!-->

<!--\end{Schunk}!-->
<p><xmp class=command>> set.seed(825)</xmp></p>
<p><xmp class=command>> rdaFit <- train(Class ~ ., data = training, </xmp></p>
<p><xmp class=command>>                  method = "rda", </xmp></p>
<p><xmp class=command>>                  trControl = fitControl, </xmp></p>
<p><xmp class=command>>                  tuneLength = 4,</xmp></p>
<p><xmp class=command>>                  metric = "ROC")</xmp></p>
<p><xmp class=command>> rdaFit</xmp></p>
<!-- begin{Schunk} !--> 
<pre>157 samples</pre> <pre> 60 predictors</pre> <pre>  2 classes: 'M', 'R' </pre> <pre></pre> <pre>No pre-processing</pre> <pre>Resampling: Cross-Validation (10 fold, repeated 10 times) </pre> <pre></pre> <pre>Summary of sample sizes: 142, 142, 140, 142, 142, 141, ... </pre> <pre></pre> <pre>Resampling results across tuning parameters:</pre> <pre></pre> <pre>  gamma  lambda  ROC    Sens   Spec   ROC SD  Sens SD  Spec SD</pre> <pre>  0      0       0.563  0.924  0.395  0.22    0.0828   0.214  </pre> <pre>  0      0.333   0.836  0.844  0.728  0.122   0.134    0.174  </pre> <pre>  0      0.667   0.858  0.825  0.78   0.114   0.148    0.158  </pre> <pre>  0      1       0.849  0.775  0.765  0.0983  0.158    0.169  </pre> <pre>  0.333  0       0.894  0.869  0.748  0.0827  0.115    0.167  </pre> <pre>  0.333  0.333   0.913  0.899  0.78   0.0696  0.112    0.138  </pre> <pre>  0.333  0.667   0.908  0.911  0.769  0.0721  0.103    0.141  </pre> <pre>  0.333  1       0.867  0.83   0.776  0.0858  0.136    0.141  </pre> <pre>  0.667  0       0.885  0.87   0.733  0.0855  0.111    0.165  </pre> <pre>  0.667  0.333   0.892  0.892  0.729  0.0852  0.101    0.161  </pre> <pre>  0.667  0.667   0.885  0.883  0.742  0.0892  0.112    0.157  </pre> <pre>  0.667  1       0.856  0.787  0.773  0.0915  0.14     0.16   </pre> <pre>  1      0       0.722  0.659  0.646  0.132   0.172    0.203  </pre> <pre>  1      0.333   0.727  0.659  0.649  0.127   0.172    0.204  </pre> <pre>  1      0.667   0.729  0.662  0.649  0.127   0.173    0.203  </pre> <pre>  1      1       0.731  0.666  0.646  0.127   0.173    0.204  </pre> <pre></pre> <pre>ROC was used to select the optimal model using  the largest value.</pre> <pre>The final values used for the model were gamma = 0.333 and lambda = 0.333. </pre>
<!--\end{Schunk}!-->
<p>
Given these models, can we make statistical statements about their
performance differences? To do this, we first collect the resampling
results using <code>resamples</code>. 
</p>
<!-- begin{Schunk} !-->
<!-- begin{Sinput} !-->

<p><xmp class=command>> resamps <- resamples(list(GBM = gbmFit3, SVM = svmFit, RDA = rdaFit))</xmp></p>



<p><xmp class=command>> resamps</xmp></p>


<!--\end{Sinput}!-->

<!--\end{Schunk}!-->
<!-- begin{Schunk} !--> 
<pre></pre> <pre>Call:</pre> <pre>resamples.default(x = list(GBM = gbmFit3, SVM = svmFit, RDA = rdaFit))</pre> <pre></pre> <pre>Models: GBM, SVM, RDA </pre> <pre>Number of resamples: 100 </pre> <pre>Performance metrics: ROC, Sens, Spec </pre> <pre>Time estimates for: everything, final model fit </pre>
<!--\end{Schunk}!-->
<!-- begin{Schunk} !-->
<!-- begin{Sinput} !-->

<p><xmp class=command>> summary(resamps)</xmp></p>


<!--\end{Sinput}!-->

<!--\end{Schunk}!-->
<!-- begin{Schunk} !--> 
<pre></pre> <pre>Call:</pre> <pre>summary.resamples(object = resamps)</pre> <pre></pre> <pre>Models: GBM, SVM, RDA </pre> <pre>Number of resamples: 100 </pre> <pre></pre> <pre>ROC </pre> <pre>      Min. 1st Qu. Median   Mean 3rd Qu. Max. NA's</pre> <pre>GBM 0.6429  0.8745 0.9252 0.9131  0.9683    1    0</pre> <pre>SVM 0.7321  0.9286 0.9683 0.9535  0.9896    1    0</pre> <pre>RDA 0.6032  0.8750 0.9219 0.9131  0.9643    1    0</pre> <pre></pre> <pre>Sens </pre> <pre>     Min. 1st Qu. Median   Mean 3rd Qu. Max. NA's</pre> <pre>GBM 0.625  0.8507 0.8889 0.8883       1    1    0</pre> <pre>SVM 0.500  0.8750 0.9444 0.9218       1    1    0</pre> <pre>RDA 0.625  0.8750 0.8889 0.8988       1    1    0</pre> <pre></pre> <pre>Spec </pre> <pre>      Min. 1st Qu. Median   Mean 3rd Qu. Max. NA's</pre> <pre>GBM 0.3750  0.7143 0.7500 0.7646  0.8571    1    0</pre> <pre>SVM 0.2857  0.7143 0.8571 0.7930  0.8750    1    0</pre> <pre>RDA 0.2857  0.7143 0.7500 0.7804  0.8571    1    0</pre> <pre></pre>
<!--\end{Schunk}!-->

<p>
There are several <pkg>lattice</pkg> plot methods that can be used to visualize
the resampling distributions: density plots, box-whisker plots,
scatterplot matrices and scatterplots of summary statistics. In the
latter case, the plot consists of a scatterplot between the two
models, as shown in the figure below. In the next figure, density plots
of the data are shown. In this figure,  the <i>R</i><sup>2</sup> distributions
indicate that M5 rules and MARS appear to be
similar to one another but different from the two tree-based
models. However, this pattern is inconsistent with the root
mean squared error distributions.
</p>

<p>A variety of plots can be created for the resampling results:</p>
<p><xmp class=command>> ## Box-Whisker plots </xmp></p>
<p><xmp class=command>> bwplot(resamps)</xmp></p>


<!-- begin{Schunk} !--> 
<p><br><a href="train_resample_box.pdf"><img width =300 height =180 src="train_resample_box.png"><br>(click image for larger pdf)</a><br><br></p>
<!--\end{Schunk}!-->

<p><xmp class=command>> ## Confidence intervals (note we plot only one outcome) </xmp></p>
<p><xmp class=command>> dotplot(resamps, metric = "ROC")</xmp></p>


<!-- begin{Schunk} !--> 
<p><br><a href="train_resample_ci.pdf"><img width =300 height =225 src="train_resample_ci.png"><br>(click image for larger pdf)</a><br><br></p>
<!--\end{Schunk}!-->

<p><xmp class=command>> ## xyplot can create several plots including simple </xmp></p>
<p><xmp class=command>> ## scatter plots of two models or a Balnd-Altman type </xmp></p>
<p><xmp class=command>> ## plot (shown below). See ?xyplot.resamples for more info </xmp></p>
<p><xmp class=command>> xyplot(resamps, what = "BlandAltman")</xmp></p>


<!-- begin{Schunk} !--> 
<p><br><a href="train_resample_ba.pdf"><img width =300 height =300 src="train_resample_ba.png"><br>(click image for larger pdf)</a><br><br></p>
<!--\end{Schunk}!-->

<p><xmp class=command>> ## Scatter plot matrix: </xmp></p>
<p><xmp class=command>> splom(resamps)</xmp></p>


<!-- begin{Schunk} !--> 
<p><br><a href="train_resample_scatmat.pdf"><img width =300 height =300 src="train_resample_scatmat.png"><br>(click image for larger pdf)</a><br><br></p>
<!--\end{Schunk}!-->

<p> Other visualizations are availible in <code>densityplot.resamples</code> and <code>parallel.resamples</code></p>


<p>
Since models are fit on the same versions of the training data, it
makes sense to make inferences on the differences between models. In
this way we reduce the within-resample correlation that may exist. We
can compute the differences, then use a simple <i>t</i>-test to evaluate
the null hypothesis that there is no difference between models. 
</p>
<!-- begin{Schunk} !-->
<!-- begin{Sinput} !-->

<p><xmp class=command>> difValues <- diff(resamps)</xmp></p>



<p><xmp class=command>> difValues</xmp></p>


<!--\end{Sinput}!-->

<!--\end{Schunk}!-->
<!-- begin{Schunk} !--> 
<pre></pre> <pre>Call:</pre> <pre>diff.resamples(x = resamps)</pre> <pre></pre> <pre>Models: GBM, SVM, RDA </pre> <pre>Metrics: ROC, Sens, Spec </pre> <pre>Number of differences: 3 </pre> <pre>p-value adjustment: bonferroni </pre>
<!--\end{Schunk}!-->
<!-- begin{Schunk} !-->
<!-- begin{Sinput} !-->

<p><xmp class=command>> summary(difValues)</xmp></p>


<!--\end{Sinput}!-->

<!--\end{Schunk}!-->
<!-- begin{Schunk} !--> 
<pre></pre> <pre>Call:</pre> <pre>summary.diff.resamples(object = difValues)</pre> <pre></pre> <pre>p-value adjustment: bonferroni </pre> <pre>Upper diagonal: estimates of the difference</pre> <pre>Lower diagonal: p-value for H0: difference = 0</pre> <pre></pre> <pre>ROC </pre> <pre>    GBM       SVM        RDA       </pre> <pre>GBM           -4.037e-02  3.329e-18</pre> <pre>SVM 1.951e-10             4.037e-02</pre> <pre>RDA 1         2.104e-08            </pre> <pre></pre> <pre>Sens </pre> <pre>    GBM      SVM      RDA     </pre> <pre>GBM          -0.03347 -0.01042</pre> <pre>SVM 0.008567           0.02306</pre> <pre>RDA 1.000000 0.200447         </pre> <pre></pre> <pre>Spec </pre> <pre>    GBM    SVM      RDA     </pre> <pre>GBM        -0.02839 -0.01571</pre> <pre>SVM 0.1182           0.01268</pre> <pre>RDA 1.0000 1.0000           </pre> <pre></pre>
<!--\end{Schunk}!-->

<p>This class also has some visualization methods, including:</p>
<p><xmp class=command>> ## Box-Whisker plots </xmp></p>
<p><xmp class=command>> bwplot(difValues)</xmp></p>


<!-- begin{Schunk} !--> 
<p><br><a href="train_diff_box.pdf"><img width =300 height =180 src="train_diff_box.png"><br>(click image for larger pdf)</a><br><br></p>
<!--\end{Schunk}!-->

<p><xmp class=command>> ## Confidence intervals for the differences </xmp></p>
<p><xmp class=command>> dotplot(difValues)</xmp></p>


<!-- begin{Schunk} !--> 
<p><br><a href="train_diff_ci.pdf"><img width =300 height =180 src="train_diff_ci.png"><br>(click image for larger pdf)</a><br><br></p>
<!--\end{Schunk}!-->

<h2>Custom Methods for <code>train</code></h2>

<p>
Although there are currently more than
140
  methods available to <code>train</code>, there may be the need to create
  custom model functions (e.g. testing a new model etc). One
  application of custom models would be to create diverse ensembles of
  models. For example, a set of different classification models may be
  fit to the same data and a "pick-the-winner" approach can be
  taken (or the average of the class probabilities could be used, see
  <a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471210781.html">Kuncheva (2004)</a> or <a href ="http://www.morganclaypool.com/doi/abs/10.2200/S00240ED1V01Y200912DMK002">Seni and Elder (2010)</a>). <code>train</code> already has
  a framework for resampling and tuning models and
  <code>predict.train</code> can be used to encapsulate the ensemble of
  models into one call for prediction.
</p>

<h3>How To Write Custom Methods</h3>

<p>
The user will need to deviate from the standard call in two ways:
<ul>
<li> use <code>method = "custom"</code> in the call to <code>train</code>, and </li>
<li> add the required functions for the model using <code>trainControl</code> </li>
</ul>
</p>
<p>
The <code>custom</code> argument of <code>trainControl</code> requires a list of
named functions with the following elements: <code>parameters</code>,
<code>model</code>, <code>prediction</code>, <code>prob</code> and <code>sort</code>.
</p>

<h3>The <code>parameters</code> Argument</h3>

<p>
This element is used to specify or generate the models tuning
parameters. This can be done either as a function to generate them or
a data frame of the actual parameters.
</p>
<p>Inputs:
<ul>
    <li><code>data</code>: a data frame of the training set data. The
    outcome will be in a column labeled <code>.outcome</code>. If the
    formula method for <code>train</code> was invoked, the data passed into
    this function will have been processed (i.e. dummy variables have
    been created etc).</li>
  <li><code>len</code> an optional parameter passed in form the
    <code>tuneLength</code> argument to <code>train</code></li>
</ul>
</p>

<p>Outputs: a data frame where
<ul>
    <li>all columns start with a dot </li>
    <li>there is at least one row</li>
</ul>
<p>
Instead of a function, the final data frame can be passed in
</p>

<h3>The <code>model</code> Function</h3>

<p>
This element fits the model and any other functions
(e.g. pre-processing of the data)
</p>

<p>Inputs:
<ul>
 <li> <code>data</code>: a data frame of the training set data. The outcome
  will be in a column labeled <code>.outcome</code>. If case weights were
  specified in the <code>train</code> call, these are in the column
  <code>.modelWeights</code>. If the formula method for <code>train</code> was
  invoked, the data passed into this function will have been processed
  (i.e. dummy variables have been created etc).
   <li> <code>weights</code> case weights
   <li> <code>parameter</code> a single row data frame with the current
    tuning parameter
   <li> <code>levels</code>: either <code>NULL</code> or a character vector or
    factor labels
   <li> <code>last</code> a logical vector for the final model fit with the
    selected tuning parameters and the full training set
   <li> <code>...</code> arguments passed form <code>train</code> to this function
</ul>
</p>
<p>
Outputs: a list with at least one element:
<ul>
   <li> <code>fit</code>: the object corresponding to the trained model
</ul>
</p>
<p>
Anything else can be attached to this object. If custom
pre-processing is required, this can be estimated in the <code>model</code>
function and attached to the output list. Subsequent calls to the
<code>prediction</code> and <code>probability</code> functions will have the
entire list available, so the processing can be applied to the new
data. 
</p>

<h3>The <code>prediction</code> Function</h3>

<p>
This should be a function that produces either a number vector (for
regression) or a factor (or character) vector for classification.
</p>
<p>
Inputs:
<ul>
 <li> <code>object</code>: a list with two elements resulting from the
  model function
 <li> <code>newdata</code>: a matrix or data frame of predictors to be
  processed through the model (and possibly pre-processing routine)
</ul>
</p>
<p>
The output should be either a numeric, character or factor vector. For
classification, factors are converted to character elsewhere to ensure
the proper levels are in the output.
</p>

<h3>The <code>probability</code> Argument</h3>
<p>
For classification models, this function should generate a data frame of class probabilities. For regression, a value of <code>NULL</code> can be used. 
</p>
<p> Inputs:
<ul>
 <li> <code>object</code>: a list with two elements resulting from the
  model function
 <li> <code>newdata</code>: a matrix or data frame of predictors to be
  processed through the model (and possibly pre-processing routine)
</ul>
</p>
<p>
The output should be a data frame with these characteristics:
<ul>
 <li> as many columns are factor levels
 <li> column names are the same as the factor levels and in the same order
</ul>
</p>

<h3>The <code>sort</code> Function</h3>

<p>
There are cases where multiple tuning parameters yield the same level
of performance. In these situations, <code>train</code> will choose the
parameters associated with the most simplistic model. This function
should take the grid of tuning parameters and order them from least
complex to most complex.
</p>
<p>
The input is a data frame of tuning parameters (without the
preceding dot in the name). 
</p>
<p>
The output is the same data frame sorted appropriately. 
</p>

<h3>An Example</h3>

<p>
As an example, suppose we want to test out <code>rpart</code> models where
we tune over the complexity parameter and the minimum number of
samples in a node to do further splitting (a.k.a <code>minsplit</code>).
</p>
<p>
We'll use the Sonar data in the <a href="http://cran.r-project.org/web/packages/mlbench/index.html"><strong>mlbench</strong></a> package to illustrate.
</p>
<p>
First, we would need to create a training grid with the candidate
values of <code>cp</code> and <code>minsplit</code>. When using the nominal
<code>rpart</code> method in <code>train</code>, an initial <code>rpart</code> model is
created and the unique values of the complexity parameter are obtained
from the sub-object <code>cptable</code>. We will test two values of
<code>minsplit</code>: 10 and 30. First, we get the unique
$C_p$ values for <code>minsplit = 10</code>
</p>

<p><xmp class=command>> library(rpart)</xmp></p>
<p><xmp class=command>> cpValues10 <- rpart(Class ~ ., data = training,</xmp></p>
<p><xmp class=command>>               control = rpart.control(minsplit = 10))$cptable[,"CP"]</xmp></p>
<p><xmp class=command>> cpValues30 <- rpart(Class ~ ., data = training,</xmp></p>
<p><xmp class=command>>               control = rpart.control(minsplit = 30))$cptable[,"CP"]</xmp></p>
<p><xmp class=command>> head(cpValues10)</xmp></p>
<!-- begin{Schunk} !--> 
<pre>         1          2          3          4          5          6 </pre> <pre>0.49315068 0.08219178 0.07534247 0.04109589 0.03196347 0.02054795 </pre>
<!--\end{Schunk}!-->

<p>
From these, we will create the tuning grid of 
11
canidiate models:
</p>

<p><xmp class=command>> rpartGrid <- data.frame(.cp = c(cpValues10, cpValues30),</xmp></p>
<p><xmp class=command>>                         .minsplit = </xmp></p>
<p><xmp class=command>>                         c(rep(10, length(cpValues10)), </xmp></p>
<p><xmp class=command>>                           rep(30, length(cpValues30))))</xmp></p>
<p>
We can now write a model function:
</p>

<p><xmp class=command>> modelFunc <- function(data, parameter, levels, last, ...)</xmp></p>
<p><xmp class=command>>   {</xmp></p>
<p><xmp class=command>>     library(rpart)</xmp></p>
<p><xmp class=command>>     ctrl <- rpart.control(cp = parameter$.cp,</xmp></p>
<p><xmp class=command>>                           minsplit = parameter$.minsplit)</xmp></p>
<p><xmp class=command>>     list(fit = rpart(.outcome ~ ., data = data, control = ctrl))    </xmp></p>                         
<p><xmp class=command>>   }</xmp></p>
<p>
It is a good idea to load the <a href="http://cran.r-project.org/web/packages/rpart/index.html"><strong>rpart</strong></a> package and anything else needed within the function. The prediciton function is simple:
</p>

<p><xmp class=command>> predFunc <- function(object, newdata)</xmp></p>
<p><xmp class=command>>   {</xmp></p>
<p><xmp class=command>>     library(rpart)</xmp></p>
<p><xmp class=command>>     predict(object$fit, newdata, type = "class")     </xmp></p>                       
<p><xmp class=command>>   }</xmp></p>
<p>
Sorting by complexity is somewhat subjective. Both parameters govern
how deep the tree can be. We will sort by <code>cp</code> then <code>minsplit</code>:
</p>
<!-- begin{Schunk} !-->
<!-- begin{Sinput} !-->

<p><xmp class=command>> sortFunc <- function(x) x[order(x$cp, x$minsplit), ]</xmp></p>


<!--\end{Sinput}!-->

<!--\end{Schunk}!-->
<p>
Now we can create a control object for <code>train</code>:
</p>
<p><xmp class=command>> ctrl <- trainControl(custom = list(</xmp></p>
<p><xmp class=command>>                      parameters = rpartGrid,</xmp></p>
<p><xmp class=command>>                      model = modelFunc,</xmp></p>
<p><xmp class=command>>                      prediction = predFunc,</xmp></p>
<p><xmp class=command>>                      probability = NULL,</xmp></p>
<p><xmp class=command>>                      sort = sortFunc),</xmp></p>
<p><xmp class=command>>                      method = "repeatedcv",</xmp></p>
<p><xmp class=command>>                      repeats = 10)</xmp></p>
<p><xmp class=command>> </xmp></p>
<p><xmp class=command>> set.seed(581)</xmp></p>
<p><xmp class=command>> customRpart <- train(Class ~ ., data = training, method = "custom", trControl = ctrl)</xmp></p>
<p>
The <code>predict</code>, <code>print</code>, <code>plot</code> and <code>resamples</code>
methods work with custom models. In the case of <code>plot.train</code>, the
axis and key labels will be the parameter names. However, <code>update</code>
can be used to make the labels more descriptive:
</p>

<!-- begin{Schunk} !-->
<!-- begin{Sinput} !-->

<p><xmp class=command>> rpartPlot <- plot(customRpart, scales = list(x = list(log = 10)))</xmp></p>



<p><xmp class=command>> rpartPlot <- update(rpartPlot, xlab = "Complexity Parameter")</xmp></p>



<p><xmp class=command>> rpartPlot</xmp></p>


<!--\end{Sinput}!-->

<!--\end{Schunk}!-->
<!-- begin{Schunk} !--> 
<p><br><a href="Train_Custom.pdf"><img width =300 height =240 src="Train_Custom.png"><br>(click image for larger pdf)</a><br><br></p>
<!--\end{Schunk}!-->



<div style="clear: both;">&nbsp;</div>
  </div>
  <!-- end #content -->
<div id="sidebar">
  <ul>
  <li>
  <h2>Links</h2>
  <p><a href="modelList.html"><tt>train</tt> Model List</a></p>
  </li>
  <li>
  <h2>Topics</h2>
  <ul>
      	<li><a href="index.html">Main Page</a></li>
  		<li><a href="datasets.html">Data Sets</a></li>
                <li><a href="visualizations.html">Visualizations</a></li>
                <li><a href="preprocess.html">Pre-Processing</a></li>
                <li><a href="splitting.html">Data Splitting</a></li>
                <li><a href="misc.html">Miscellaneous Model Functions</a></li>
                <li><a href="training.html">Model Training and Tuning</a></li>
                <li><a href="modelList.html"><tt>train</tt> Model List</a></li>
                 <li><a href="bytag.html"><tt>train</tt> Models By Tag</a></li>
                <li><a href="varimp.html">Variable Importance</a></li>
                <li><a href="featureselection.html">Feature Selection</a></li>
                <li><a href="other.html">Other Functions</a></li>
                <li><a href="parallel.html">Parallel Processing</a></li>
</ul>
  </li>
  </ul>
  </div>
  <!-- end #sidebar -->
<div style="clear: both;">&nbsp;</div>
  </div>
  <div class="container"><img src="images/img03.png" width="1000" height="40" alt="" /></div>
  <!-- end #page -->
</div>
  <div id="footer-content"></div>
  <div id="footer">
  <p>Created on Tue Dec 11 2012 using caret version 5.15-045 and R version 2.15.2 (2012-10-26).</p>
  </div>
  <!-- end #footer -->
</body>
  </html>
