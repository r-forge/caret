


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
    Design by Free CSS Templates
    http://www.freecsstemplates.org
    Released for free under a Creative Commons Attribution 2.5 License

    Name       : Emerald 
    Description: A two-column, fixed-width design with dark color scheme.
    Version    : 1.0
    Released   : 20120902

  -->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
<style type="text/css">.knitr.inline {
  background-color: #f7f7f7;
  border:solid 1px #B0B0B0;
}
.error {
	font-weight: bold;
	color: #FF0000;
},
.warning {
	font-weight: bold;
}
.message {
	font-style: italic;
}
.source, .output, .warning, .error, .message {
	padding: 0em 1em;
  border:solid 1px #F7F7F7;
}
.source {
  background-color: #f5f5f5;
}
.rimage.left {
  text-align: left;
}
.rimage.right {
  text-align: right;
}
.rimage.center {
  text-align: center;
}
.hl.num {
  color: #AF0F91;
}
.hl.str {
  color: #317ECC;
}
.hl.com {
  color: #AD95AF;
  font-style: italic;
}
.hl.opt {
  color: #000000;
}
.hl.std {
  color: #585858;
}
.hl.kwa {
  color: #295F94;
  font-weight: bold;
}
.hl.kwb {
  color: #B05A65;
}
.hl.kwc {
  color: #55aa55;
}
.hl.kwd {
  color: #BC5A65;
  font-weight: bold;
}</style>
    <meta name="keywords" content="" />
    <meta name="description" content="" />
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <title>Parallel Processing</title>
    <link href='http://fonts.googleapis.com/css?family=Abel' rel='stylesheet' type='text/css'>
    <link href="style.css" rel="stylesheet" type="text/css" media="screen" />
  </head>
  <body>
    <div id="wrapper">
      <div id="header-wrapper" class="container">
	<div id="header" class="container">
	  <div id="logo">
	    <h1><a href="#">Parallel Processing</a></h1>
	  </div>
          <!--
	      <div id="menu">
		<ul>
		  <li class="current_page_item"><a href="#">Homepage</a></li>
		  <li><a href="#">Blog</a></li>
		  <li><a href="#">Photos</a></li>
		  <li><a href="#">About</a></li>
		  <li><a href="#">Contact</a></li>
		</ul>
	      </div>
              -->
	</div>
	<div><img src="images/img03.png" width="1000" height="40" alt="" /></div>
      </div>
      <!-- end #header -->
      <div id="page">
	<div id="content">
	

<p>
In this package, resampling is primary approach for optimizing predictive models with tuning parameters. To do this, many alternate versions of the training set are used to train the model and predict a hold-out set. This process is repeated many times to get performance estimates that generalize to new data sets. Each of the resampled data sets is independent of the others, so there is no formal requirement that the models must be run sequentially. If a computer with multiple processors or cores is available, the computations could be spread across these "workers" to increase the computational efficiency. <a href="http://cran.r-project.org/web/packages/caret/index.html"><strong>caret</strong></a> leverages one of the parallel processing frameworks in R to do just this. The <a href="http://cran.r-project.org/web/packages/foreach/index.html"><strong>foreach</strong></a> package allows R code to be run either sequentially or in parallel using several different technologies, such as the <a href="http://cran.r-project.org/web/packages/multicore/index.html"><strong>multicore</strong></a> or <a href="http://cran.r-project.org/web/packages/Rmpi/index.html"><strong>Rmpi</strong></a> packages (see <a href="http://www.jstatsoft.org/v31/i01/paper">Schmidberger <i>et al</i>, 2009 </a> for summaries and descriptions of the available options). There are several R packages that work with <a href="http://cran.r-project.org/web/packages/foreach/index.html"><strong>foreach</strong></a>  to implement these techniques, such as <a href="http://cran.r-project.org/web/packages/doMC/index.html"><strong>doMC</strong></a> (for <a href="http://cran.r-project.org/web/packages/multicore/index.html"><strong>multicore</strong></a>) or <a href="http://cran.r-project.org/web/packages/doMPI/index.html"><strong>doMPI</strong></a> (for <a href="http://cran.r-project.org/web/packages/Rmpi/index.html"><strong>Rmpi</strong></a>). 
</p>
<p>
To tune a predictive model using multiple workers, the function syntax in the <a href="http://cran.r-project.org/web/packages/caret/index.html"><strong>caret</strong></a> package functions (e.g. <span class="mx funCall">train</span>, <span class="mx funCall">rfe</span> or <span class="mx funCall">sbf</span>) do not change. A separate function is used to "register" the parallel processing technique and specify the number of workers to use. For example, to use the <a href="http://cran.r-project.org/web/packages/multicore/index.html"><strong>multicore</strong></a>) package (not available on Windows) with five cores on the same machine, the package is loaded and them registered:
</p>
<div class="chunk" id="unnamed-chunk-2"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">library</span><span class="hl std">(doMC)</span>
<span class="hl kwd">registerDoMC</span><span class="hl std">(</span><span class="hl kwc">cores</span> <span class="hl std">=</span> <span class="hl num">5</span><span class="hl std">)</span>
<span class="hl com">## All subsequent models are then run in parallel</span>
<span class="hl std">model</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">train</span><span class="hl std">(y</span> <span class="hl opt">~</span> <span class="hl std">.,</span> <span class="hl kwc">data</span> <span class="hl std">= training,</span> <span class="hl kwc">method</span> <span class="hl std">=</span> <span class="hl str">&quot;rf&quot;</span><span class="hl std">)</span>
</pre></div>
</div></div>

<p>
The syntax for other packages associated with <a href="http://cran.r-project.org/web/packages/foreach/index.html"><strong>foreach</strong></a> is very similar. Note that as the number of workers increases, the memory required also increase. For example, using five workers would keep a total of six versions of the data in memory. If the data are large or the computational model is demanding, performance can be affected if the amount of required memory exceeds the physical amount available. Also, for <span class="mx funCall">rfe</span> and <span class="mx funCall">sbf</span>, these functions may call <span class="mx funCall">train</span> for some models. In this case, registering <i>M</i> workers will actually invoke <i>M</i><sup>2</sup> total processes.
</p>
<p>
Does this help reduce the time to fit models? A moderately sized data set (4331 rows and 8) was modeled multiple times
with different number of workers for several models. Random forest was
used with 2000 trees and tuned over 10 values of <i>m</i><sub>try</sub>. Variable
importance calculations were also conducted during each model
fit. Linear discriminant analysis was also run, as was a
cost-sensitive radial basis function support vector machine (tuned
over 15 cost values). All models were tuned using five repeats of
10-fold cross-validation. The results are shown in the figure below. The y-axis corresponds to the total execution
time (encompassing model tuning and the final model fit) versus the
number of workers. Random forest clearly took the longest to train and
the LDA models were very computationally efficient. The total time (in
minutes) decreased as the number of workers increase but stabilized
around seven workers. The data for this plot were generated in a
randomized fashion so that there should be no bias in the run
order. The bottom right panel shows the <i>speed-up</i> which is the
sequential time divided by the parallel time. For example, a speed-up
of three indicates that the parallel version was three times faster
than the sequential version. At best, parallelization can achieve
linear speed-ups; that is, for <i>M</i> workers, the parallel time is
1/<i>M</i>. For these models, the speed-up is close to linear until four
or five workers are used. After this, there is a small improvement in
performance.  Since LDA is already computationally efficient, the
speed-up levels off more rapidly than the other models. While not
linear, the decrease in execution time is helpful - a nearly 10 hour
model fit was decreased to about 90 minutes.
</p>
<p><img width = 600 height =450 src="parallel.png"></p>
<p>
Note that some models, especially those using the <a href="http://cran.r-project.org/web/packages/RWeka/index.html"><strong>RWeka</strong></a> package, may not be able to be run in parallel due to the underlying code structure. 
</p>
<p>
<span class="mx funCall">train</span>, <span class="mx funCall">rfe</span>, <span class="mx funCall">sbf</span>, <span class="mx funCall">bag</span> and
<span class="mx funCall">avNNet</span> were given an additional argument in their respective
control files called <span class="mx arg">allowParallel</span> that defaults to
<code>TRUE</code>. When <code>TRUE</code>, the code will be executed in parallel
if a parallel backend (e.g. <strong>doMC</strong>) is registered. When
<span class="mx arg">allowParallel</span><code> = FALSE</code>, the parallel backend is always
ignored. The use case is when <span class="mx funCall">rfe</span> or <span class="mx funCall">sbf</span> calls
<span class="mx funCall">train</span>. If a parallel backend with <i>P</i> processors is being used,
the combination of these functions will create <i>P</i><sup>2</sup> processes. Since
some operations benefit more from parallelization than others, the
user has the ability to concentrate computing resources for specific
functions.
<p>
One additional "trick" that <span class="mx funCall">train</span> exploits to increase computational efficiency is to use sub-models; a single model fit can produce predictions for multiple tuning parameters. For example, in most implementations of boosted models, a model trained on <i>B</i> boosting iterations can produce predictions for models for iterations less than <i>B</i>. Suppose a <span class="mx funCall">gbm</span> model was tuned over the following grid:
</p>


<div class="chunk" id="unnamed-chunk-3"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">gbmGrid</span> <span class="hl kwb">&lt;-</span>  <span class="hl kwd">expand.grid</span><span class="hl std">(</span><span class="hl kwc">.interaction.depth</span> <span class="hl std">=</span> <span class="hl kwd">c</span><span class="hl std">(</span><span class="hl num">1</span><span class="hl std">,</span> <span class="hl num">5</span><span class="hl std">,</span> <span class="hl num">9</span><span class="hl std">),</span>
                        <span class="hl kwc">.n.trees</span> <span class="hl std">= (</span><span class="hl num">1</span><span class="hl opt">:</span><span class="hl num">15</span><span class="hl std">)</span><span class="hl opt">*</span><span class="hl num">100</span><span class="hl std">,</span>
                        <span class="hl kwc">.shrinkage</span> <span class="hl std">=</span> <span class="hl num">0.1</span><span class="hl std">)</span>
</pre></div>
</div></div>

    



<p>
In reality, <span class="mx funCall">train</span> only created objects for 3 models and derived the other predictions from these objects. This trick is used for the following models: <code>bagEarth</code>, <code>blackboost</code>, <code>bstLs</code>, <code>bstSm</code>, <code>bstTree</code>, <code>C5.0</code>, <code>C5.0Cost</code>, <code>cubist</code>, <code>earth</code>, <code>enet</code>, <code>foba</code>, <code>gamboost</code>, <code>gbm</code>, <code>glmboost</code>, <code>glmnet</code>, <code>kernelpls</code>, <code>lars</code>, <code>lars2</code>, <code>lasso</code>, <code>lda2</code>, <code>leapBackward</code>, <code>leapForward</code>, <code>leapSeq</code>, <code>LogitBoost</code>, <code>pam</code>, <code>partDSA</code>, <code>pcr</code>, <code>PenalizedLDA</code>, <code>pls</code>, <code>relaxo</code>, <code>rpart</code>, <code>rpart2</code>, <code>rpartCost</code>, <code>simpls</code>, <code>superpc</code>, <code>widekernelpls</code>.
</p>

	  <div style="clear: both;">&nbsp;</div>
	</div>
	<!-- end #content -->
	<div id="sidebar">
	  <ul>
	    <li>
	      <h2>Links</h2>
	      <p><a href="modelList.html"><tt>train</tt> Model List</a></p>
	    </li>
	    <li>
	      <h2>Topics</h2>
	      <ul>
        <li><a href="index.html">Main Page</a></li>        
  		<li><a href="datasets.html">Data Sets</a></li>
                <li><a href="visualizations.html">Visualizations</a></li>
                <li><a href="preprocess.html">Pre-Processing</a></li>
                <li><a href="splitting.html">Data Splitting</a></li>
                <li><a href="misc.html">Miscellaneous Model Functions</a></li>
                <li><a href="training.html">Model Training and Tuning</a></li>
                <li><a href="modelList.html"><tt>train</tt> Model List</a></li>
                <li><a href="bytag.html"><tt>train</tt> Models By Tag</a></li>
                 <li><a href="similarity.html"><tt>train</tt> Models By Similarity</a></li>
                <li><a href="custom_models.html">Using Custom Models</a></li>
                <li><a href="varimp.html">Variable Importance</a></li>
                <li><a href="featureselection.html">Feature Selection</a></li>
                <li><a href="other.html">Other Functions</a></li>
                <li><a href="parallel.html">Parallel Processing</a></li>
                <li><a href="adaptive.html">Adaptive Resampling</a></li> 
	      </ul>
	    </li>
	  </ul>
	</div>
	<!-- end #sidebar -->
	<div style="clear: both;">&nbsp;</div>
      </div>
      <div class="container"><img src="images/img03.png" width="1000" height="40" alt="" /></div>
      <!-- end #page -->
    </div>
    <div id="footer-content"></div>
    <div id="footer">
      <p>Created on Sat May 31 2014 using caret version 6.0-29 and R version 3.0.3 (2014-03-06).</p>
    </div>
    <!-- end #footer -->
  </body>
</html>
